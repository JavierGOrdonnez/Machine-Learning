{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter magic commands\n",
    "\n",
    "Use %lsmagic to see all magic commands.\n",
    "\n",
    "Use %command to use it.\n",
    "\n",
    "Use %%command to use it accross the cell\n",
    "\n",
    "Use ?command to find out what that command does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import sklearn as sk\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace missing data by the mean of the column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for pandas\n",
    "empty = data.apply(lambda col: pd.isnull(col)).sum() # just detects which ones are empty. Not necessary\n",
    "data['Column'].fillna(data['Column'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for Numpy arrays\n",
    "# --> I have still to come across it / develop it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into training and test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X is all data; rows = n_samples and columns = features. Y is the labels associated to them (shape = (n_samples,))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "# We have to divide both observations and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "transformer = StandardScaler() # Define the object\n",
    "transformer.fit(X_train)  # fit does nothing, just learns mean and std from training data\n",
    "X_train_norm = transformer.transform(X_train)\n",
    "X_test_norm =  transformer.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: both test and training data are normalized with the same mean and std, that of the training data, so they are normalized in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "- **Training MSE**: $$MSE_{train} = \\frac{1}{N_{train}} \\sum_{i=1}^{N_{train}} \\left(y^{(i)}-f({\\bf x_{train}}^{(i)})\\right)^2$$\n",
    "\n",
    "- **Test MSE**: \\begin{align}\n",
    "MSE_{test} =  \\frac{1}{N_{test}}\\sum_{i=1}^{N_{test}} \\left(y^{(i)}-f({\\bf x_{test}}^{(i)})\\right)^2\n",
    "\\end{align}\n",
    "\n",
    "Note that we are interested in evaluating how well our data **generalizes to data we have never seen**. Therefore, **the test database should NEVER be used** at any stage of the training, nor during the selection of the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cross validation\n",
    "In data cross validation, the training data is split in training and validation data iteratively, each time changing which subset of data is used for validation. Then, the results are averaged.\n",
    "\n",
    "Also, this is done over different (hyper)parameters values in order to find their optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_dictionary = {'n_neighbors' : np.arange(1,40)}\n",
    "model = KNNreg() # model in which we want to optimize the hyperparameters\n",
    "# cv = 10 means that a 10 fold cross validation is performed\n",
    "# That is, training data is divided in 10 subsets and each is used as validation once, over 10 different trials\n",
    "cross_val = GridSearchCV(model,parameters,iid=False,cv=10,scoring= 'neg_mean_squared_error')\n",
    "# Before, we just defined it. With .fit, it iterates over the data\n",
    "cross_val.fit(X_train,Y_train) # this executes the cross-validation\n",
    "# The results are the following:\n",
    "optimal_estimator = cross_val.best_estimator_ # model already trained and optimal parameters\n",
    "dict_parameters = cross_val.best_params_ # dictionary containing the optimal values of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a KNN regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors = 5)\n",
    "neigh.fit(X_train,Y_train)\n",
    "y_test_predicted = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, it is needed to define our NN as a class, inheriting from nn.Module\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self,dimx):\n",
    "        super().__init__() # needed to inherit        \n",
    "        # Define the nn.Parameters, i.e. the values to be optimized\n",
    "        self.weights = nn.Parameter(torch.randn(dimx,1),requires_grad = True)        \n",
    "        self.bias = nn.Parameter(torch.randn(1,1),requires_grad = True)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):  # define the forward function. In this example, a simple sigmoid\n",
    "        # Pass the input tensor through each of our operations\n",
    "        p = self.sigmoid(torch.matmul(x,self.weights)+self.bias)\n",
    "        return p\n",
    "    \n",
    "my_classifier = LR(x.shape[1]) # Instantiate the NN. \n",
    "# Remember thn the __init__ method requires dimx, the dimension (number of features) of the data\n",
    "criterion = nn.BCELoss() # define a binary cross entropy as loss function\n",
    "output = my_classifier.forward(torch.tensor(x))\n",
    "loss = criterion(output,torch.tensor(y))  # it is a scalar value\n",
    "# But contains the information to compute the gradients of the operations and parameters that led to such value\n",
    "loss.backward() # Through the backward operator, the gradient for each parameter is computed and stored in x.grad\n",
    "# If we were to perform .backward again, gradients get added (not overwritten)\n",
    "# So it is necessary to set them to zero before using it again:\n",
    "my_classifier.zero_grad()\n",
    "\n",
    "# Now, it would be needed to iterate and optimize with respect to the gradient\n",
    "# That has to be done with the specific optimizer library we are using, with a .step() function or sth similar\n",
    "# Of course, it has to be iterated in a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
