{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter magic commands\n",
    "\n",
    "Use %lsmagic to see all magic commands.\n",
    "\n",
    "Use %command to use it.\n",
    "\n",
    "Use %%command to use it accross the cell\n",
    "\n",
    "Use ?command to find out what that command does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import sklearn as sk\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace missing data by the mean of the column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for pandas\n",
    "empty = data.apply(lambda col: pd.isnull(col)).sum() # just detects which ones are empty. Not necessary\n",
    "data['Column'].fillna(data['Column'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for Numpy arrays\n",
    "# --> I have still to come across it / develop it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into training and test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X is all data; rows = n_samples and columns = features. Y is the labels associated to them (shape = (n_samples,))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "# We have to divide both observations and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "transformer = StandardScaler() # Define the object\n",
    "transformer.fit(X_train)  # fit does nothing, just learns mean and std from training data\n",
    "X_train_norm = transformer.transform(X_train)\n",
    "X_test_norm =  transformer.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: both test and training data are normalized with the same mean and std, that of the training data, so they are normalized in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "- **Training MSE**: $$MSE_{train} = \\frac{1}{N_{train}} \\sum_{i=1}^{N_{train}} \\left(y^{(i)}-f({\\bf x_{train}}^{(i)})\\right)^2$$\n",
    "\n",
    "- **Test MSE**: \\begin{align}\n",
    "MSE_{test} =  \\frac{1}{N_{test}}\\sum_{i=1}^{N_{test}} \\left(y^{(i)}-f({\\bf x_{test}}^{(i)})\\right)^2\n",
    "\\end{align}\n",
    "\n",
    "Note that we are interested in evaluating how well our data **generalizes to data we have never seen**. Therefore, **the test database should NEVER be used** at any stage of the training, nor during the selection of the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cross validation\n",
    "In data cross validation, the training data is split in training and validation data iteratively, each time changing which subset of data is used for validation. Then, the results are averaged.\n",
    "\n",
    "Also, this is done over different (hyper)parameters values in order to find their optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_dictionary = {'n_neighbors' : np.arange(1,40)}\n",
    "model = KNNreg() # model in which we want to optimize the hyperparameters\n",
    "# cv = 10 means that a 10 fold cross validation is performed\n",
    "# That is, training data is divided in 10 subsets and each is used as validation once, over 10 different trials\n",
    "cross_val = GridSearchCV(model,parameters,iid=False,cv=10,scoring= 'neg_mean_squared_error')\n",
    "# Before, we just defined it. With .fit, it iterates over the data\n",
    "cross_val.fit(X_train,Y_train) # this executes the cross-validation\n",
    "# The results are the following:\n",
    "optimal_estimator = cross_val.best_estimator_ # model already trained and optimal parameters\n",
    "dict_parameters = cross_val.best_params_ # dictionary containing the optimal values of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a KNN regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors = 5)\n",
    "neigh.fit(X_train,Y_train)\n",
    "y_test_predicted = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-layer NN (logistic regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, it is needed to define our NN as a class, inheriting from nn.Module\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self,dimx):\n",
    "        super().__init__() # needed to inherit        \n",
    "        # Define the nn.Parameters, i.e. the values to be optimized\n",
    "        self.weights = nn.Parameter(torch.randn(dimx,1),requires_grad = True)        \n",
    "        self.bias = nn.Parameter(torch.randn(1,1),requires_grad = True)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):  # define the forward function. In this example, a simple sigmoid\n",
    "        # Pass the input tensor through each of our operations\n",
    "        p = self.sigmoid(torch.matmul(x,self.weights)+self.bias)\n",
    "        return p\n",
    "    \n",
    "my_classifier = LR(x.shape[1]) # Instantiate the NN. \n",
    "# Remember thn the __init__ method requires dimx, the dimension (number of features) of the data\n",
    "criterion = nn.BCELoss() # define a binary cross entropy as loss function\n",
    "output = my_classifier.forward(torch.tensor(x))\n",
    "loss = criterion(output,torch.tensor(y))  # it is a scalar value\n",
    "# But contains the information to compute the gradients of the operations and parameters that led to such value\n",
    "loss.backward() # Through the backward operator, the gradient for each parameter is computed and stored in x.grad\n",
    "# If we were to perform .backward again, gradients get added (not overwritten)\n",
    "# So it is necessary to set them to zero before using it again:\n",
    "my_classifier.zero_grad()\n",
    "\n",
    "# Now, it would be needed to iterate and optimize with respect to the gradient\n",
    "# That has to be done with the specific optimizer library we are using, with a .step() function or sth similar\n",
    "# Of course, it has to be iterated in a for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer dense NN (including training and evaluation methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNNvalidation(nn.Module):\n",
    "  def __init__(self,dim_input,nlabels,epochs=10,learning_rate=0.001):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dimx = dim_input\n",
    "    self.nlabels = nlabels\n",
    "    self.epochs = epochs\n",
    "    self.lr = learning_rate\n",
    "\n",
    "    self.layer1 = nn.Linear(dim_input,256)\n",
    "    self.layer2 = nn.Linear(256,128)\n",
    "    self.layer3 = nn.Linear(128,64)\n",
    "    self.layer4 = nn.Linear(64,nlabels)\n",
    "\n",
    "    self.optim = optim.Adam(self.parameters(), self.lr)\n",
    "    self.criterion = nn.NLLLoss()  \n",
    "    self.relu = nn.ReLU()\n",
    "    self.logsoftmax = nn.LogSoftmax(dim=1)  \n",
    "\n",
    "    self.train_loss_during_training = [] \n",
    "    self.valid_loss_during_training = []\n",
    "    self.valid_acc_during_training = []\n",
    "    self.train_acc_during_training = []\n",
    "\n",
    "    self.epoch_time = []\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    x = self.layer1(x)\n",
    "    x = self.relu(x)\n",
    "\n",
    "    x = self.layer2(x)\n",
    "    x = self.relu(x)\n",
    "\n",
    "    x = self.layer3(x)\n",
    "    x = self.relu(x)\n",
    "\n",
    "    x = self.layer4(x)\n",
    "    x = self.logsoftmax(x)  \n",
    "    return x\n",
    "\n",
    "  def trainloop(self, trainloader, validloader):\n",
    "    for e in range(int(self.epochs)):\n",
    "      t_start = time.time()\n",
    "      epoch_error = 0.\n",
    "      acc = 0.\n",
    "      for images, labels in trainloader:\n",
    "        self.optim.zero_grad()\n",
    "        out = self.forward(images.view(images.shape[0], -1))\n",
    "        loss = self.criterion(out,labels)\n",
    "        loss.backward() \n",
    "        self.optim.step()\n",
    "        epoch_error += loss.item()\n",
    "\n",
    "        top_pos, top_class = out.topk(1,dim=1) # find the position (and label) for the most probable label for each datapoint\n",
    "        equals = (top_class == labels.view(images.shape[0],1))\n",
    "        acc += torch.sum(equals.type(torch.FloatTensor))\n",
    "      self.train_loss_during_training.append(epoch_error / len(trainloader))\n",
    "      self.train_acc_during_training.append(acc / len(trainloader.dataset.data))\n",
    "\n",
    "      # Testing accuracy and loss over validation dataset\n",
    "      with torch.no_grad():            \n",
    "        running_loss = 0.\n",
    "        acc = 0.\n",
    "        for images,labels in validloader:           \n",
    "          out = self.forward(images.view(images.shape[0], -1))\n",
    "          loss = self.criterion(out,labels)\n",
    "          running_loss += loss.item()\n",
    "          top_pos, top_class = out.topk(1,dim=1) # find the position (and label) for the most probable label for each datapoint\n",
    "          equals = (top_class == labels.view(images.shape[0],1))\n",
    "          acc += torch.sum(equals.type(torch.FloatTensor))\n",
    "        self.valid_loss_during_training.append(running_loss / len(validloader))\n",
    "        self.valid_acc_during_training.append(acc / len(validloader.dataset.data))\n",
    "        t_end = time.time()\n",
    "        self.epoch_time.append(t_end-t_start)\n",
    "\n",
    "      print(\"Epoch %d: Training loss is %f and validation loss is %f. Elapsed time: %f\" %(e,self.train_loss_during_training[-1],self.valid_loss_during_training[-1],self.epoch_time[-1]))\n",
    "      \n",
    "  def evaluation(self,dataloader):\n",
    "    acc = 0\n",
    "    with torch.no_grad():\n",
    "      for images,labels in dataloader:\n",
    "        output = self.forward(images.view(images.shape[0],-1))  # output is a series of prob (one prob per possible label, per object --> from logsoftmax)\n",
    "        top_pos, top_class = output.topk(1,dim=1) # find the position (and label) for the most probable label for each datapoint\n",
    "        equals = (top_class == labels.view(images.shape[0],1))\n",
    "        acc += torch.sum(equals.type(torch.FloatTensor))\n",
    "      return acc / len(dataloader.dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer dense NN including dropout and saving method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNN_dropout(myNNvalidation_save):\n",
    "  def __init__(self,dim_input,nlabels, dropout_prob=0.5, epochs=10,learning_rate=0.0005, savepath='/content/drive/My Drive/checkpoints'):\n",
    "    super().__init__(dim_input,nlabels,epochs,learning_rate, savepath)\n",
    "    self.dropout_prob = dropout_prob\n",
    "    self.dropout = nn.Dropout(p=self.dropout_prob) # Dropout module\n",
    "\n",
    "  # Redefine the forward method to include dropout\n",
    "  def forward(self, x):\n",
    "\n",
    "    x = self.layer1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    x = self.layer2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    x = self.layer3(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    x = self.layer4(x)\n",
    "    x = self.logsoftmax(x)  \n",
    "    return x   \n",
    "\n",
    "  # Redefine the training loop to set evaluation mode on and off\n",
    "  def trainloop(self, trainloader, validloader):\n",
    "    for e in range(int(self.epochs)):\n",
    "      self.train()  # set training mode       ################################\n",
    "      t_start = time.time()\n",
    "      epoch_error = 0.\n",
    "      acc = 0.\n",
    "      for images, labels in trainloader:\n",
    "        self.optim.zero_grad()\n",
    "        out = self.forward(images.view(images.shape[0], -1))\n",
    "        loss = self.criterion(out,labels)\n",
    "        loss.backward() \n",
    "        self.optim.step()\n",
    "        epoch_error += loss.item()\n",
    "\n",
    "        top_pos, top_class = out.topk(1,dim=1) # find the position (and label) for the most probable label for each datapoint\n",
    "        equals = (top_class == labels.view(images.shape[0],1))\n",
    "        acc += torch.sum(equals.type(torch.FloatTensor))\n",
    "      self.train_loss_during_training.append(epoch_error / len(trainloader))\n",
    "      self.train_acc_during_training.append(acc / len(trainloader.dataset.data))\n",
    "\n",
    "      # Testing accuracy and loss over validation dataset\n",
    "      with torch.no_grad(): \n",
    "        self.eval()  # set in evaluation mode    ################################     \n",
    "        running_loss = 0.\n",
    "        acc = 0.\n",
    "        for images,labels in validloader:           \n",
    "          out = self.forward(images.view(images.shape[0], -1))\n",
    "          loss = self.criterion(out,labels)\n",
    "          running_loss += loss.item()\n",
    "          top_pos, top_class = out.topk(1,dim=1) # find the position (and label) for the most probable label for each datapoint\n",
    "          equals = (top_class == labels.view(images.shape[0],1))\n",
    "          acc += torch.sum(equals.type(torch.FloatTensor))\n",
    "        self.valid_loss_during_training.append(running_loss / len(validloader))\n",
    "        self.valid_acc_during_training.append(acc / len(validloader.dataset.data))\n",
    "        self.train()  # set back in training mode  ###############################\n",
    "\n",
    "      # Let's include automatic saving in all iterations\n",
    "      self.save_during_training(path = self.savepath, epoch_number = e, valid_loss = self.valid_loss_during_training[-1])\n",
    "      t_end = time.time()\n",
    "      self.epoch_time.append(t_end-t_start)\n",
    "      print(\"Epoch %d: Training loss is %f and validation loss is %f. Elapsed time: %f\" %(e,self.train_loss_during_training[-1],self.valid_loss_during_training[-1],self.epoch_time[-1]))\n",
    "\n",
    "  # Redefine the evaluation method to set evaluation mode\n",
    "  def evaluation(self,dataloader):\n",
    "    acc = 0.\n",
    "    self.eval()   ################################################################\n",
    "    with torch.no_grad():\n",
    "      for images,labels in dataloader:\n",
    "        output = self.forward(images.view(images.shape[0],-1))  # output is a series of prob (one prob per possible label, per object --> from logsoftmax)\n",
    "        top_pos, top_class = output.topk(1,dim=1) # find the position (and label) for the most probable label for each datapoint\n",
    "        equals = (top_class == labels.view(images.shape[0],1))\n",
    "        acc += torch.sum(equals.type(torch.FloatTensor))\n",
    "    self.train()\n",
    "    return acc / len(dataloader.dataset.data)\n",
    "\n",
    "  def save_during_training(self, path, epoch_number, valid_loss = 0.000):\n",
    "    import datetime\n",
    "    currentDT = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    nn_name = 'myNNdropout'\n",
    "    savename = '{0}/{1}_checkpoint_{2}_epoch{3:d}_validloss{4:.4f}.pth'.format(path,nn_name,currentDT,epoch_number,valid_loss)\n",
    "    torch.save(self.state_dict(), savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet5_extended_GPU(Lenet5_extended):\n",
    "    def __init__(self,####):\n",
    "        super().__init__(####)  \n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Define the device, in the GPU\n",
    "\n",
    "        self.to(self.device)\n",
    "        # Put all the model in the GPU\n",
    "            \n",
    "    def trainloop(self,trainloader,validloader):\n",
    "        for e in range(int(self.epochs)):\n",
    "            #####################\n",
    "            for images, labels in trainloader:\n",
    "                \n",
    "              # Move input and label tensors to the default device\n",
    "              images, labels = images.to(self.device), labels.to(self.device) \n",
    "              # Move the data to the GPU (so that data does not have to be transfered all the time) \n",
    "\n",
    "              ###########################################\n",
    "              ###########################################\n",
    "                        \n",
    "            # Turn off gradients for validation, saves memory and computations\n",
    "            with torch.no_grad():\n",
    "\n",
    "              for images,labels in validloader:\n",
    "            \n",
    "                # Move input and label tensors to the default device\n",
    "                images, labels = images.to(self.device), labels.to(self.device)               \n",
    "                \n",
    "                ###########################################\n",
    "                ###########################################\n",
    "\n",
    "            print(\"Epoch %d: Training loss is %f and validation loss is %f. Elapsed time: %f\" %(e,self.train_loss_during_training[-1],self.valid_loss_during_training[-1],self.epoch_time[-1]))\n",
    "\n",
    "    def evaluation(self,dataloader):\n",
    "      with torch.no_grad():\n",
    "            \n",
    "        for images,labels in dataloader:\n",
    "          # Move input and label tensors to the default device\n",
    "          images, labels = images.to(self.device), labels.to(self.device) \n",
    "            \n",
    "          ##########################################################\n",
    "    \n",
    "        return (eval_loss/len(dataloader) , acc / len(dataloader) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-save and load methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_during_training(self): # save weights (state_dict)\n",
    "    import datetime\n",
    "    currentDT = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    nn_name = 'Lenet5_densedropout'\n",
    "    savename = '{0}/{1}_checkpoint_{2}_epoch{3:d}_validloss{4:.4f}.pth'.format(self.savepath,nn_name,currentDT,self.current_epoch,self.valid_loss_during_training[-1])\n",
    "    torch.save(self.state_dict(), savename)\n",
    "    # save training data (acc and loss over training)\n",
    "    filename = self.savepath + '/training_information'\n",
    "    my_vars = [self.current_epoch, self.train_loss_during_training, self.valid_loss_during_training, self.valid_acc_during_training, self.train_acc_during_training, self.epoch_time]\n",
    "    with open(filename,'wb') as f:\n",
    "      pickle.dump(my_vars, f)\n",
    "    \n",
    "def load(self, path, filename):\n",
    "    state_dict = torch.load(path+'/'+filename+'.pth')\n",
    "    self.load_state_dict(state_dict)\n",
    "    with open(path+'/training_information','rb') as f:\n",
    "      self.current_epoch, self.train_loss_during_training, self.valid_loss_during_training, self.valid_acc_during_training, self.train_acc_during_training, self.epoch_time = pickle.load(f)\n",
    "    return self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
