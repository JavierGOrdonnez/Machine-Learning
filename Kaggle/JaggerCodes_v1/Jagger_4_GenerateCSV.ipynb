{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Jagger_4_GenerateCSV.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v65uQSS7bUYT",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pUbv9aojbUYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import _pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "import time\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMvCy_NBbUYp",
        "colab_type": "text"
      },
      "source": [
        "# Data loading\n",
        "Remember to change path if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoI5WFfsbUY4",
        "colab_type": "text"
      },
      "source": [
        "# Data split & Spectrum in regular bins\n",
        "Data is split between a proper training set (later used in cross-validation), and a test_train set, which will help us in determining under/overfitting as we do have labels for them.\n",
        "\n",
        "Spectrums are divided in regular size bins, always the same, so that we can treat them as features, not worrying about different mz scales. According to the literature the peaks contain the relevant information, then we only save the maximum value in the bin (range of mz coordinates) so that peak information is never lost. Moreover, by performing this regularization in bins, peaks at very close mz values (same compound, small mz differences due to experimental uncertainty) are seen by the machine as belonging to the same bin and therefore the same feature. Therefore, it facilitates to use peaks as values.\n",
        "\n",
        "Also, peak values are normalized by the maximum peak value of the spectrum, as specific values are experiment-dependent and do not carry information, only the relation between peak sizes does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pb4OK1vtbUY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spectrum_in_bins(df,m,M,bin_size):\n",
        "    # Now, let's define the mz ranges, and the label associated to each of them (the mean of the limiting values of each bin)\n",
        "    range_min = []; range_max = []; range_label = [];\n",
        "    for mz in range(m,M,bin_size):\n",
        "        range_min.append(mz)\n",
        "        range_max.append(mz+bin_size)\n",
        "        range_label.append(np.mean([range_min[-1],range_max[-1]]).astype(int))\n",
        "\n",
        "\n",
        "    N = len(df)  # number of samples\n",
        "    L = len(range_min)  # length of new spectrum (number of bins)\n",
        "    all_data = np.zeros((N,L))\n",
        "    for idx in range(N): \n",
        "        intensity = df[['intensity']].iloc[idx].values[0]\n",
        "        mzcoord   = df[['coord_mz']].iloc[idx].values[0]\n",
        "        idx_data_in_bins = np.zeros((1,L))\n",
        "        for i,mz in enumerate(range_min):\n",
        "            intensity_range = intensity[(mzcoord > mz) & (mzcoord < (mz+bin_size))]\n",
        "            if len(intensity_range) > 0 :\n",
        "                idx_data_in_bins[0,i] = np.max(intensity_range)\n",
        "            else: # if those mz coordinates are not in that spectrum\n",
        "                idx_data_in_bins[0,i] = 0   \n",
        "\n",
        "        # Normalize the amplitude of the spectrum\n",
        "        idx_data_in_bins[0,:] = idx_data_in_bins[0,:] / np.max(idx_data_in_bins[0,:])\n",
        "        \n",
        "        \n",
        "        all_data[idx,:] = idx_data_in_bins\n",
        "    new_df = pd.DataFrame(data=all_data, columns = range_label, index = df.index)\n",
        "    return new_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOx7ayitoDt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters of the spectrum processing\n",
        "m = 2000; M = 20000; \n",
        "bin_size = 50;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXRNNH88qMfE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a3a36eca-b391-4ac0-f8a4-d3e18312411d"
      },
      "source": [
        "targets_full_train = targets\n",
        "spectrum_full_train = spectrum_in_bins(data, m, M, bin_size)\n",
        "print('Spectrum regularized!')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spectrum regularized!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSFVS5i0prL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Let's create manually a list of good classifiers\n",
        "clf_oxalacina       = SVC(kernel='rbf',class_weight='balanced', C=10000, gamma=0.1, probability=True)\n",
        "clf_amikacina       = LogisticRegression(max_iter=1e6,solver='lbfgs',class_weight='balanced', C=100000)\n",
        "clf_amoxiclav       = LogisticRegression(max_iter=1e6,solver='lbfgs',class_weight='balanced', C=100)\n",
        "clf_ciprofloxacino  = LogisticRegression(max_iter=1e6,solver='lbfgs',class_weight='balanced', C=100)\n",
        "clf_clindamicina    = KNeighborsClassifier(n_neighbors=16)\n",
        "clf_eritromicina    = LogisticRegression(max_iter=1e6,solver='lbfgs',class_weight='balanced', C=1000)\n",
        "clf_levofloxacino   = LogisticRegression(max_iter=1e6,solver='lbfgs',class_weight='balanced', C=10000)\n",
        "clf_penicilina      = LogisticRegression(max_iter=1e6,solver='lbfgs',class_weight='balanced', C=100000)\n",
        "clf_tobramicina     = LogisticRegression(max_iter=1e6,solver='lbfgs',class_weight='balanced', C=10000)\n",
        "\n",
        "clf_list = [clf_oxalacina, clf_amikacina, clf_amoxiclav, clf_ciprofloxacino, clf_clindamicina,\n",
        "            clf_eritromicina, clf_levofloxacino, clf_penicilina, clf_tobramicina]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he4cLalLoszo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_csv_from_clf(clf_list,path):\n",
        "  # classifiers must be provided with parameters, and in a list [clf_antibiotic0, clf_antibiotic1, ...]\n",
        "  # spectrum and targets full train (containing all training points) will be used for training the clfs in clf list\n",
        "  # df_test must be provided as loaded from the file\n",
        "\n",
        "  # read all data from files\n",
        "  zf = zipfile.ZipFile(path+'zipped_TrainData.zip', 'r')\n",
        "  df_full_train = _pickle.loads(zf.open('TrainData.pkl').read());   zf.close()\n",
        "\n",
        "  zf = zipfile.ZipFile(path+'zipped_TestDataUnlabeled.zip', 'r')\n",
        "  df_test = _pickle.loads(zf.open('TestDataUnlabeled.pkl').read());   zf.close()\n",
        "\n",
        "  # Process test df to get UNIQUE samples and convert to spectrum\n",
        "  df_unique_test = df_test.drop_duplicates(subset='ID_sample')\n",
        "  spectrum_test_forcsv = spectrum_in_bins(df_unique_test,m,M,bin_size)\n",
        "  print('Spectrum regularized!')\n",
        "\n",
        "  # Process train set to later train the clfs\n",
        "  spectrum_full_train = spectrum_in_bins(df_full_train.iloc[:,-2:],m,M,bin_size)\n",
        "  targets_full_train  = df_full_train.iloc[:,1:-2]  \n",
        "\n",
        "  # read the submission example file\n",
        "  df_submission = pd.read_csv(path+'SubmissionSample.csv') \n",
        "  categories = df_submission.columns[1:]\n",
        "  df_submission['ID']= df_unique_test['ID_sample'].values\n",
        "\n",
        "  for c, cat in enumerate(categories): \n",
        "    # clean NaN values\n",
        "      if (df_full_train[cat].isnull().sum() > 0).all(): # if there are NaN values, we should remove those samples\n",
        "          merged_full_train = pd.concat([spectrum_full_train , targets_full_train],axis=1,copy=True)\n",
        "          clean_full_train = merged_full_train.dropna(subset=[cat])\n",
        "          print('Dropped ',len(merged_full_train)-len(clean_full_train),' from training set')\n",
        "\n",
        "          Y_train = clean_full_train.iloc[:,-9 + c].to_numpy().reshape(-1,)\n",
        "          X_train = clean_full_train.iloc[:,:-9]\n",
        "      else:\n",
        "          Y_train = targets_full_train.iloc[:,c].to_numpy().reshape(-1,)\n",
        "          X_train = spectrum_full_train.copy(deep=True)\n",
        "\n",
        "      # Use a Log-regressor as classifier   \n",
        "      clf_base = clf_list[c].fit(X_train,Y_train)\n",
        "      # Compute its test prestiction and save this output\n",
        "      o_test = clf_base.predict_proba(spectrum_test_forcsv)[:,1]\n",
        "      df_submission[cat] = o_test\n",
        "\n",
        "  # Save the dataframe with the predicted outputs\n",
        "  df_submission.set_index('ID').to_csv('SubmissionBaseline_new_firsttrial.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA-YvcNzos7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "964ca383-6fa5-480f-b923-c2274ac7a3b4"
      },
      "source": [
        "generate_csv_from_clf(clf_list,path)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spectrum regularized!\n",
            "Dropped  88  from training set\n",
            "Dropped  4  from training set\n",
            "Dropped  82  from training set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G14cAXUlos4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}