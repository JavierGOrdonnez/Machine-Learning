{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import _pickle\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "Remember to change path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2000; M = 12000; \n",
    "bin_size = 5;\n",
    "\n",
    "path = \"D:/GitHub/Machine-Learning/Kaggle/\"\n",
    "# path = \"C:/Users/Javi/Documents/GitHub/Machine-Learning/Kaggle/\"\n",
    "\n",
    "# # Take the data from Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive',force_remount=True)\n",
    "# path = \"/content/drive/My Drive/Colab Notebooks/Kaggle/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = path + 'Kaggle classifiers/bin size 5/'\n",
    "path_results = path + 'Kaggle_results/'\n",
    "\n",
    "ncpu = os.cpu_count()\n",
    "if (ncpu>1): njobs = ncpu - 1; \n",
    "else: njobs = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile(path+'zipped_TrainData.zip', 'r')\n",
    "df_train = _pickle.loads(zf.open('TrainData.pkl').read())\n",
    "zf.close()\n",
    "\n",
    "zf = zipfile.ZipFile(path+'zipped_TestDataUnlabeled.zip', 'r')\n",
    "df_test = _pickle.loads(zf.open('TestDataUnlabeled.pkl').read())\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split & Spectrum in regular bins\n",
    "Data is split between a proper training set (later used in cross-validation), and a test_train set, which will help us in determining under/overfitting as we do have labels for them.\n",
    "\n",
    "Spectrums are divided in regular size bins, always the same, so that we can treat them as features, not worrying about different mz scales. According to the literature the peaks contain the relevant information, then we only save the maximum value in the bin (range of mz coordinates) so that peak information is never lost. Moreover, by performing this regularization in bins, peaks at very close mz values (same compound, small mz differences due to experimental uncertainty) are seen by the machine as belonging to the same bin and therefore the same feature. Therefore, it facilitates to use peaks as values.\n",
    "\n",
    "Also, peak values are normalized by the maximum peak value of the spectrum, as specific values are experiment-dependent and do not carry information, only the relation between peak sizes does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spectrum_in_bins(df,m,M,bin_size):\n",
    "    # Now, let's define the mz ranges, and the label associated to each of them (the mean of the limiting values of each bin)\n",
    "    range_min = []; range_max = []; range_label = [];\n",
    "    for mz in range(m,M,bin_size):\n",
    "        range_min.append(mz)\n",
    "        range_max.append(mz+bin_size)\n",
    "        range_label.append(np.mean([range_min[-1],range_max[-1]]).astype(int))\n",
    "    N = len(df)  # number of samples\n",
    "    L = len(range_min)  # length of new spectrum (number of bins)\n",
    "    all_data = np.zeros((N,L))\n",
    "    for idx in range(N): \n",
    "        intensity = df[['intensity']].iloc[idx].values[0]\n",
    "        mzcoord   = df[['coord_mz']].iloc[idx].values[0]\n",
    "        idx_data_in_bins = np.zeros((1,L))\n",
    "        for i,mz in enumerate(range_min):\n",
    "            intensity_range = intensity[(mzcoord > mz) & (mzcoord < (mz+bin_size))]\n",
    "            if len(intensity_range) > 0 :\n",
    "                idx_data_in_bins[0,i] = np.max(intensity_range)\n",
    "            else: # if those mz coordinates are not in that spectrum\n",
    "                idx_data_in_bins[0,i] = 0   \n",
    "\n",
    "        # Normalize the amplitude of the spectrum\n",
    "        idx_data_in_bins[0,:] = idx_data_in_bins[0,:] / np.max(idx_data_in_bins[0,:])\n",
    "        all_data[idx,:] = idx_data_in_bins\n",
    "    new_df = pd.DataFrame(data=all_data, columns = range_label, index = df.index)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 412 and test_train samples: 104\n",
      "Spectrum regularized!\n"
     ]
    }
   ],
   "source": [
    "# Extract data (spectra) and targets of the df_train set\n",
    "data = df_train.iloc[:,-2:]\n",
    "targets = df_train.iloc[:,1:-2]\n",
    "\n",
    "# Then, split into a train and test_train set\n",
    "data_train, data_test_train, targets_train, targets_test_train = train_test_split(data, targets, test_size=0.2, random_state=42) # split the data\n",
    "print('Training samples: '+str(len(data_train))+' and test_train samples: ' + str(len(data_test_train)) )\n",
    "\n",
    "# apply the bins to all spectra, so that our feature space becomes the same for all samples (make them regular, all the same)\n",
    "spectrum_train = spectrum_in_bins(data_train,m,M,bin_size)\n",
    "spectrum_test_train = spectrum_in_bins(data_test_train,m,M,bin_size)\n",
    "print('Spectrum regularized!')\n",
    "# these spectrum_... are our X for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different classifiers\n",
    "The try_clf function has been built for, given a classifier and a parameter dictionary (for hyperparameter cross-validation), create a classifier for each antibiotic, and return the results. This enables for fast testing of different classifiers. Moreover, the function also takes charge of suppressing NaN values in the targets ocurring for amikacina, levofloxacino and tobramicina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nan_samples(spectrum,targets, c, cat):\n",
    "# if there are any NaN values, we should remove those samples\n",
    "    if (targets[cat].isnull().sum() > 0).all(): \n",
    "        merged = pd.concat([spectrum , targets],axis=1,copy=True)\n",
    "        clean = merged.dropna(subset=[cat])\n",
    "        Y = clean.iloc[:,-9+c].to_numpy().reshape(-1,)\n",
    "        X = clean.iloc[:,:-9]\n",
    "\n",
    "    else:\n",
    "        Y = targets.iloc[:,c].to_numpy().reshape(-1,)\n",
    "        X = spectrum.copy(deep=True)\n",
    "    return X , Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def try_clf(clf,params,n_cv=5):  # new version! (after Sevilla)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    best_classifiers = [];\n",
    "    grid_list = [];\n",
    "    AUC_train = []; AUC_test_train = [];\n",
    "    \n",
    "    categories = targets_train.columns[:]    \n",
    "    for c,cat in enumerate(categories):\n",
    "\n",
    "        print([cat]) # indicate in which antibiotic we are\n",
    "        \n",
    "        # Selection of train and test data (depending on whether there are NaN target values)\n",
    "        X_train, Y_train = clean_nan_samples(spectrum_train,targets_train, c, cat)\n",
    "        X_test_train, Y_test_train = clean_nan_samples(spectrum_test_train,targets_test_train, c, cat)\n",
    "            \n",
    "        # perform a GridSearchCV in order to train a classifier for this antibiotic\n",
    "        grid = GridSearchCV(clf,param_grid=params,scoring='roc_auc',n_jobs=njobs,pre_dispatch='2*n_jobs', cv=n_cv, iid=False,return_train_score=True)\n",
    "        grid.fit(X_train, Y_train)\n",
    "\n",
    "        # print the best parameters (to detect edge values), and save that classifier\n",
    "        print('The best parameters are: ',grid.best_params_)\n",
    "        best_clf = grid.best_estimator_\n",
    "        best_classifiers.append(best_clf)\n",
    "        grid_list.append(grid)\n",
    "        \n",
    "        # compute the AUC of the classifier\n",
    "        if callable(getattr(best_clf,\"predict_proba\",None)):\n",
    "            pred_train = best_clf.predict_proba(X_train)[:,-1] # only take last column, the prob of Y = +1\n",
    "            pred_test = best_clf.predict_proba(X_test_train)[:,-1]\n",
    "        else:\n",
    "            print('Using decision_function instead of predict_proba')\n",
    "            pred_train = best_clf.decision_function(X_train)\n",
    "            pred_test = best_clf.decision_function(X_test_train)            \n",
    "        auc_score_train = roc_auc_score(Y_train, pred_train)\n",
    "        auc_score_test = roc_auc_score(Y_test_train, pred_test)\n",
    "        print('Train AUC: ',np.round(auc_score_train,4),' and test_train AUC: ',np.round(auc_score_test,4))\n",
    "        AUC_train.append(auc_score_train)\n",
    "        AUC_test_train.append(auc_score_test)\n",
    "        \n",
    "    avg_AUC_train = np.mean(AUC_train)\n",
    "    avg_AUC_test_train = np.mean(AUC_test_train)\n",
    "    print('\\n\\nThe average train AUC is',np.round(avg_AUC_train,4),'and the avg test_train AUC is',np.round(avg_AUC_test_train,4))\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print('\\nFull execution took ',np.round(t2-t1,1),'seconds')\n",
    "    print('\\nDONE!')\n",
    "    return best_classifiers, grid_list, AUC_train, AUC_test_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l1_clfs():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(penalty='l1',solver='liblinear',max_iter=1e6, class_weight='balanced')\n",
    "    params = {'C':10.**np.arange(0,3)} # only up to 100, so that enough little features remain\n",
    "    l1_best_clfs,_,_,_ = try_clf(clf,params)\n",
    "    return l1_best_clfs\n",
    "\n",
    "def obtain_l1_vects(l1_best_clfs,spectrum_train,targets_train):\n",
    "    l1_feat_list = []    \n",
    "    categories = targets_train.columns[:]\n",
    "    \n",
    "    for c, cat in enumerate(categories):\n",
    "        n = np.sum(np.abs(l1_best_clfs[c].coef_) > 0)\n",
    "        print('Number of features:',n)\n",
    "        while n == 0:\n",
    "            clf = l1_best_clfs[c]\n",
    "            c_value = clf.get_params()['C']\n",
    "            new_c = c_value * 10\n",
    "            clf.set_params(C=new_c)\n",
    "            X_train, Y_train = clean_nan_samples(spectrum_train,targets_train, c, cat)            \n",
    "            clf.fit(X_train, Y_train) # refit with higher C\n",
    "            l1_best_clfs[c] = clf\n",
    "            n = np.sum(np.abs(clf.coef_) > 0)\n",
    "            print(n)\n",
    "    \n",
    "        # once we know we have at least one non-zero feature\n",
    "        vect = (np.abs(l1_best_clfs[c].coef_)>0).reshape(-1,)\n",
    "        l1_feat_list.append(vect)\n",
    "    return l1_feat_list\n",
    "\n",
    "# to be applyied to each category\n",
    "def apply_l1_feature_selection(spectrum_train,vect): # vect is l1_feat_list[c]\n",
    "    new_spectrum = spectrum_train.copy(deep=True).iloc[:,vect]   \n",
    "    return new_spectrum\n",
    "\n",
    "# to obtain a list, with an element for each category\n",
    "def apply_l1_feature_selection_listmode(spectrum_train,vect_list): # vect is l1_feat_list\n",
    "    new_spectrum_list = []\n",
    "    categories = targets_train.columns[:]\n",
    "    for c,cat in enumerate(categories):\n",
    "        new_spectrum = spectrum_train.copy(deep=True).iloc[:,vect_list[c]]   \n",
    "        new_spectrum_list.append(new_spectrum)\n",
    "    return new_spectrum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OXACILINA']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train AUC:  0.9989  and test_train AUC:  0.8164\n",
      "['AMIKACINA']\n",
      "The best parameters are:  {'C': 10.0}\n",
      "Train AUC:  0.9509  and test_train AUC:  0.8278\n",
      "['AMOXI/CLAV']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train AUC:  0.9992  and test_train AUC:  0.7957\n",
      "['CIPROFLOXACINO']\n",
      "The best parameters are:  {'C': 10.0}\n",
      "Train AUC:  0.9495  and test_train AUC:  0.7524\n",
      "['CLINDAMICINA']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train AUC:  0.9991  and test_train AUC:  0.631\n",
      "['ERITROMICINA']\n",
      "The best parameters are:  {'C': 10.0}\n",
      "Train AUC:  0.9337  and test_train AUC:  0.6787\n",
      "['LEVOFLOXACINO']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train AUC:  0.9994  and test_train AUC:  0.802\n",
      "['PENICILINA']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train AUC:  1.0  and test_train AUC:  0.6447\n",
      "['TOBRAMICINA']\n",
      "The best parameters are:  {'C': 10.0}\n",
      "Train AUC:  0.9513  and test_train AUC:  0.8388\n",
      "\n",
      "\n",
      "The average train AUC is 0.9758 and the avg test_train AUC is 0.7542\n",
      "\n",
      "Full execution took  46.7 seconds\n",
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "l1_best_clfs = get_l1_clfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 149\n",
      "Number of features: 66\n",
      "Number of features: 139\n",
      "Number of features: 75\n",
      "Number of features: 154\n",
      "Number of features: 87\n",
      "Number of features: 146\n",
      "Number of features: 100\n",
      "Number of features: 66\n"
     ]
    }
   ],
   "source": [
    "l1_feat_list = obtain_l1_vects(l1_best_clfs,spectrum_train,targets_train)\n",
    "new_spectrum_list = apply_l1_feature_selection_listmode(spectrum_train,l1_feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_spectre(spectre, df):\n",
    "  # Include the ID_sample column for the group_by\n",
    "  spectre['ID_sample'] = df.ID_sample\n",
    "  # MEAN OF THE SPECTRE\n",
    "  spectre = spectre.groupby('ID_sample').mean().reset_index()\n",
    "  return spectre\n",
    "\n",
    "# to be applyied to each category\n",
    "def apply_l1_feature_selection(spectrum_train,vect): # vect is l1_feat_list[c]\n",
    "    new_spectrum = spectrum_train.copy(deep=True).iloc[:,vect]   \n",
    "    return new_spectrum\n",
    "\n",
    "def generate_csv_from_clf(clf_list, path, path_results, file_name, L1_FEATURE_SELECTION=False, feature_vector_list = None):\n",
    "  # classifiers must be provided with parameters, and in a list [clf_antibiotic0, clf_antibiotic1, ...]\n",
    "  # spectrum and targets full train (containing all training points) will be used for training the clfs in clf list\n",
    "  # df_test must be provided as loaded from the file\n",
    "\n",
    "  # read all data from files\n",
    "  zf = zipfile.ZipFile(path+'zipped_TrainData.zip', 'r')\n",
    "  df_full_train = _pickle.loads(zf.open('TrainData.pkl').read());   zf.close()\n",
    "\n",
    "  zf = zipfile.ZipFile(path+'zipped_TestDataUnlabeled.zip', 'r')\n",
    "  df_test = _pickle.loads(zf.open('TestDataUnlabeled.pkl').read());   zf.close()\n",
    "\n",
    "  # Process test df to get UNIQUE samples and convert to spectrum\n",
    "\n",
    "  # df_unique_test = df_test.drop_duplicates(subset='ID_sample')\n",
    "\n",
    "  spectrum_test_forcsv = spectrum_in_bins(df_test,m,M,bin_size)\n",
    "  spectrum_test_forcsv = get_unique_spectre(spectrum_test_forcsv, df_test)\n",
    "    \n",
    "  # Process train set to later train the clfs\n",
    "  spectrum_full_train = spectrum_in_bins(df_full_train.iloc[:,-2:],m,M,bin_size)\n",
    "  targets_full_train  = df_full_train.iloc[:,1:-2]  \n",
    "\n",
    "  # read the submission example file\n",
    "  df_submission = pd.read_csv(path+'SubmissionSample.csv') \n",
    "  categories = df_submission.columns[1:]\n",
    "  df_submission['ID']= spectrum_test_forcsv['ID_sample'].values\n",
    "  # To eliminate the ID_sample from the spectrum\n",
    "  spectrum_test_forcsv = spectrum_test_forcsv.drop(columns=['ID_sample'])\n",
    "  for c, cat in enumerate(categories): \n",
    "      # clean NaN values\n",
    "      X_train, Y_train = clean_nan_samples(spectrum_full_train,targets_full_train, c, cat)\n",
    "\n",
    "      if L1_FEATURE_SELECTION: # a boolean that decides whether to apply L1 feature selection (L1 feature list has to be already defined, and input to the function)\n",
    "        X_train= apply_l1_feature_selection(X_train,feature_vector_list[c])\n",
    "        spectrum_test_forcsv = apply_l1_feature_selection(spectrum_test_forcsv,feature_vector_list[c])\n",
    "\n",
    "      # fit the classifier\n",
    "      clf_base = clf_list[c].fit(X_train,Y_train)\n",
    "      # Compute its test prestiction and save this output\n",
    "    \n",
    "    \n",
    "      o_test = clf_base.predict_proba(spectrum_test_forcsv)[:,1]\n",
    "#       o_test = clf_base.predict_proba(X_test)\n",
    "        \n",
    "      df_submission[cat] = o_test\n",
    "\n",
    "  # Save the dataframe with the predicted outputs\n",
    "  df_submission = df_submission.set_index('ID')\n",
    "  df_submission.to_csv(path_results + file_name + '.csv')\n",
    "  print('DONE!')\n",
    "  return df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n",
      "File: L1_LogReg has been successfully generated\n"
     ]
    }
   ],
   "source": [
    "name = 'L1_LogReg'\n",
    "df_submission = generate_csv_from_clf(l1_best_clfs,path,path_results, name)\n",
    "print('File: '+name+' has been successfully generated')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
