{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import _pickle\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "Remember to change path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"D:/GitHub/Machine-Learning/Kaggle/\"\n",
    "# path = \"C:/Users/Javi/Documents/GitHub/Machine-Learning/Kaggle/\"\n",
    "\n",
    "zf = zipfile.ZipFile(path+'zipped_TrainData.zip', 'r')\n",
    "df_train = _pickle.loads(zf.open('TrainData.pkl').read())\n",
    "zf.close()\n",
    "\n",
    "zf = zipfile.ZipFile(path+'zipped_TestDataUnlabeled.zip', 'r')\n",
    "df_test = _pickle.loads(zf.open('TestDataUnlabeled.pkl').read())\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split & Spectrum in regular bins\n",
    "Data is split between a proper training set (later used in cross-validation), and a test_train set, which will help us in determining under/overfitting as we do have labels for them.\n",
    "\n",
    "Spectrums are divided in regular size bins, always the same, so that we can treat them as features, not worrying about different mz scales. According to the literature the peaks contain the relevant information, then we only save the maximum value in the bin (range of mz coordinates) so that peak information is never lost. Moreover, by performing this regularization in bins, peaks at very close mz values (same compound, small mz differences due to experimental uncertainty) are seen by the machine as belonging to the same bin and therefore the same feature. Therefore, it facilitates to use peaks as values.\n",
    "\n",
    "Also, peak values are normalized by the maximum peak value of the spectrum, as specific values are experiment-dependent and do not carry information, only the relation between peak sizes does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spectrum_in_bins(df,m,M,bin_size):\n",
    "    # Now, let's define the mz ranges, and the label associated to each of them (the mean of the limiting values of each bin)\n",
    "    range_min = []; range_max = []; range_label = [];\n",
    "    for mz in range(m,M,bin_size):\n",
    "        range_min.append(mz)\n",
    "        range_max.append(mz+bin_size)\n",
    "        range_label.append(np.mean([range_min[-1],range_max[-1]]).astype(int))\n",
    "\n",
    "\n",
    "    N = len(df)  # number of samples\n",
    "    L = len(range_min)  # length of new spectrum (number of bins)\n",
    "    all_data = np.zeros((N,L))\n",
    "    for idx in range(N): \n",
    "        intensity = df[['intensity']].iloc[idx].values[0]\n",
    "        mzcoord   = df[['coord_mz']].iloc[idx].values[0]\n",
    "        idx_data_in_bins = np.zeros((1,L))\n",
    "        for i,mz in enumerate(range_min):\n",
    "            intensity_range = intensity[(mzcoord > mz) & (mzcoord < (mz+bin_size))]\n",
    "            if len(intensity_range) > 0 :\n",
    "                idx_data_in_bins[0,i] = np.max(intensity_range)\n",
    "            else: # if those mz coordinates are not in that spectrum\n",
    "                idx_data_in_bins[0,i] = 0   \n",
    "\n",
    "        # Normalize the amplitude of the spectrum\n",
    "        idx_data_in_bins[0,:] = idx_data_in_bins[0,:] / np.max(idx_data_in_bins[0,:])\n",
    "        \n",
    "        \n",
    "        all_data[idx,:] = idx_data_in_bins\n",
    "    new_df = pd.DataFrame(data=all_data, columns = range_label, index = df.index)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 412 and test_train samples: 104\n",
      "Spectrum regularized!\n"
     ]
    }
   ],
   "source": [
    "# Extract data (spectra) and targets of the df_train set\n",
    "data = df_train.iloc[:,-2:]\n",
    "targets = df_train.iloc[:,1:-2]\n",
    "\n",
    "m = 2000; M = 20000; \n",
    "bin_size = 50;\n",
    "\n",
    "# Then, split into a train and test_train set\n",
    "data_train, data_test_train, targets_train, targets_test_train = train_test_split(data, targets, test_size=0.2, random_state=42) # split the data\n",
    "print('Training samples: '+str(len(data_train))+' and test_train samples: ' + str(len(data_test_train)) )\n",
    "\n",
    "# apply the bins to all spectra, so that our feature space becomes the same for all samples (make them regular, all the same)\n",
    "spectrum_train = spectrum_in_bins(data_train,m,M,bin_size)\n",
    "spectrum_test_train = spectrum_in_bins(data_test_train,m,M,bin_size)\n",
    "print('Spectrum regularized!')\n",
    "# these spectrum_... are our X for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different classifiers\n",
    "The try_clf function has been built for, given a classifier and a parameter dictionary (for hyperparameter cross-validation), create a classifier for each antibiotic, and return the results. This enables for fast testing of different classifiers. Moreover, the function also takes charge of suppressing NaN values in the targets ocurring for amikacina, levofloxacino and tobramicina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nan_samples(spectrum,targets, c, cat):\n",
    "# if there are any NaN values, we should remove those samples\n",
    "    if (targets[cat].isnull().sum() > 0).all(): \n",
    "#         print('There are NaN values in',cat)\n",
    "        merged = pd.concat([spectrum , targets],axis=1,copy=True)\n",
    "        clean = merged.dropna(subset=[cat])\n",
    "#         print('Dropped ',len(merged)-len(clean))\n",
    "\n",
    "        Y = clean.iloc[:,-9+c].to_numpy().reshape(-1,)\n",
    "        X = clean.iloc[:,:-9]\n",
    "\n",
    "    else:\n",
    "        Y = targets.iloc[:,c].to_numpy().reshape(-1,)\n",
    "        X = spectrum.copy(deep=True)\n",
    "    return X , Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def try_clf(clf,params,n_cv=5):\n",
    "    t1 = time.time()\n",
    "    \n",
    "    best_classifiers = [];\n",
    "    accuracies_train = []; accuracies_test_train = [];\n",
    "    AUC_train = []; AUC_test_train = [];\n",
    "    \n",
    "    categories = targets_train.columns[:]    \n",
    "    for c,cat in enumerate(categories):\n",
    "\n",
    "        print([cat]) # indicate in which antibiotic we are\n",
    "        \n",
    "        # Selection of train and test data (depending on whether there are NaN target values)\n",
    "        X_train, Y_train = clean_nan_samples(spectrum_train,targets_train, c, cat)\n",
    "        X_test_train, Y_test_train = clean_nan_samples(spectrum_test_train,targets_test_train, c, cat)\n",
    "            \n",
    "        # perform a GridSearchCV in order to train a classifier for this antibiotic\n",
    "        grid = GridSearchCV(clf,param_grid=params, cv=n_cv, iid=False)\n",
    "        grid.fit(X_train, Y_train)\n",
    "\n",
    "        # print the best parameters (to detect edge values), and save that classifier\n",
    "        print('The best parameters are: ',grid.best_params_)\n",
    "        best_clf = grid.best_estimator_\n",
    "        best_classifiers.append(best_clf)\n",
    "        \n",
    "        # compute the accuracy of the classifier\n",
    "        acc_train = best_clf.score(X_train, Y_train)\n",
    "        acc_test = best_clf.score(X_test_train, Y_test_train)\n",
    "        print('Train accuracy: ',np.round(acc_train,4),' and test_train accuracy: ',np.round(acc_test,4))\n",
    "        accuracies_train.append(acc_train)\n",
    "        accuracies_test_train.append(acc_test)\n",
    "        \n",
    "        # compute the AUC of the classifier\n",
    "        if callable(getattr(best_clf,\"predict_proba\",None)):\n",
    "            pred_train = best_clf.predict_proba(X_train)[:,-1] # only take last column, the prob of Y = +1\n",
    "            pred_test = best_clf.predict_proba(X_test_train)[:,-1]\n",
    "        else:\n",
    "            print('Using decision_function instead of predict_proba')\n",
    "            pred_train = best_clf.decision_function(X_train)\n",
    "            pred_test = best_clf.decision_function(X_test_train)            \n",
    "        auc_score_train = roc_auc_score(Y_train, pred_train)\n",
    "        auc_score_test = roc_auc_score(Y_test_train, pred_test)\n",
    "        print('Train AUC: ',np.round(auc_score_train,4),' and test_train AUC: ',np.round(auc_score_test,4))\n",
    "        AUC_train.append(auc_score_train)\n",
    "        AUC_test_train.append(auc_score_test)\n",
    "        \n",
    "    avg_AUC_train = np.mean(AUC_train)\n",
    "    avg_AUC_test_train = np.mean(AUC_test_train)\n",
    "    print('\\n\\nThe average train AUC is',np.round(avg_AUC_train,4),'and the avg test_train AUC is',np.round(avg_AUC_test_train,4))\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print('\\nFull execution took ',np.round(t2-t1,1),'seconds')\n",
    "    print('\\nDONE!')\n",
    "    return best_classifiers, accuracies_train, accuracies_test_train, AUC_train, AUC_test_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l1_clfs():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(penalty='l1',solver='liblinear',max_iter=1e6, class_weight='balanced')\n",
    "    params = {'C':10.**np.arange(-2,5)}\n",
    "    l1_best_clfs,_,_,_,_ = try_clf(clf,params)\n",
    "    return l1_best_clfs\n",
    "\n",
    "def obtain_l1_vects(l1_best_clfs,spectrum_train,targets_train):\n",
    "    l1_feat_list = []    \n",
    "    categories = targets_train.columns[:]\n",
    "    \n",
    "    for c, cat in enumerate(categories):\n",
    "        n = np.sum(np.abs(l1_best_clfs[c].coef_) > 0)\n",
    "        print('Number of features:',n)\n",
    "        while n == 0:\n",
    "            clf = l1_best_clfs[c]\n",
    "            c_value = clf.get_params()['C']\n",
    "            new_c = c_value * 10\n",
    "            clf.set_params(C=new_c)\n",
    "            X_train, Y_train = clean_nan_samples(spectrum_train,targets_train, c, cat)            \n",
    "            clf.fit(X_train, Y_train) # refit with higher C\n",
    "            l1_best_clfs[c] = clf\n",
    "            n = np.sum(np.abs(clf.coef_) > 0)\n",
    "            print(n)\n",
    "    \n",
    "        # once we know we have at least one non-zero feature\n",
    "        vect = (np.abs(l1_best_clfs[c].coef_)>0).reshape(-1,)\n",
    "        l1_feat_list.append(vect)\n",
    "    return l1_feat_list\n",
    "\n",
    "# to be applyied to each category\n",
    "def apply_l1_feature_selection(spectrum_train,vect): # vect is l1_feat_list[c]\n",
    "    new_spectrum = spectrum_train.copy(deep=True).iloc[:,vect]   \n",
    "    return new_spectrum\n",
    "\n",
    "# to obtain a list, with an element for each category\n",
    "def apply_l1_feature_selection_listmode(spectrum_train,vect_list): # vect is l1_feat_list\n",
    "    new_spectrum_list = []\n",
    "    categories = targets_train.columns[:]\n",
    "    for c,cat in enumerate(categories):\n",
    "        new_spectrum = spectrum_train.copy(deep=True).iloc[:,vect_list[c]]   \n",
    "        new_spectrum_list.append(new_spectrum)\n",
    "    return new_spectrum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OXACILINA']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.9248  and test_train accuracy:  0.7788\n",
      "Train AUC:  0.9742  and test_train AUC:  0.8348\n",
      "['AMIKACINA']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.9104  and test_train accuracy:  0.7927\n",
      "Train AUC:  0.9734  and test_train AUC:  0.7327\n",
      "['AMOXI/CLAV']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.9175  and test_train accuracy:  0.7596\n",
      "Train AUC:  0.9744  and test_train AUC:  0.815\n",
      "['CIPROFLOXACINO']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.8762  and test_train accuracy:  0.7596\n",
      "Train AUC:  0.9552  and test_train AUC:  0.8066\n",
      "['CLINDAMICINA']\n",
      "The best parameters are:  {'C': 0.01}\n",
      "Train accuracy:  0.7961  and test_train accuracy:  0.7788\n",
      "Train AUC:  0.5  and test_train AUC:  0.5\n",
      "['ERITROMICINA']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.8398  and test_train accuracy:  0.6731\n",
      "Train AUC:  0.9266  and test_train AUC:  0.7207\n",
      "['LEVOFLOXACINO']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.9122  and test_train accuracy:  0.7745\n",
      "Train AUC:  0.9677  and test_train AUC:  0.7873\n",
      "['PENICILINA']\n",
      "The best parameters are:  {'C': 10000.0}\n",
      "Train accuracy:  1.0  and test_train accuracy:  0.9038\n",
      "Train AUC:  1.0  and test_train AUC:  0.8489\n",
      "['TOBRAMICINA']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.906  and test_train accuracy:  0.7711\n",
      "Train AUC:  0.9724  and test_train AUC:  0.7627\n",
      "\n",
      "\n",
      "The average train AUC is 0.916 and the avg test_train AUC is 0.7565\n",
      "\n",
      "Full execution took  1837.8 seconds\n",
      "\n",
      "DONE!\n",
      "Number of features: 93\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'l1_feat_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5f76faedf81b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ml1_best_clfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_l1_clfs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ml1_feat_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobtain_l1_vects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml1_best_clfs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspectrum_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnew_spectrum_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_l1_feature_selection_listmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspectrum_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml1_feat_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-33a248e5bca8>\u001b[0m in \u001b[0;36mobtain_l1_vects\u001b[1;34m(l1_best_clfs, spectrum_train, targets_train)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# once we know we have at least one non-zero feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml1_best_clfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0ml1_feat_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ml1_feat_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l1_feat_list' is not defined"
     ]
    }
   ],
   "source": [
    "# l1_best_clfs = get_l1_clfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 93\n",
      "Number of features: 82\n",
      "Number of features: 94\n",
      "Number of features: 99\n",
      "Number of features: 0\n",
      "9\n",
      "Number of features: 104\n",
      "Number of features: 101\n",
      "Number of features: 129\n",
      "Number of features: 84\n"
     ]
    }
   ],
   "source": [
    "l1_feat_list = obtain_l1_vects(l1_best_clfs,spectrum_train,targets_train)\n",
    "new_spectrum_list = apply_l1_feature_selection_listmode(spectrum_train,l1_feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      "  True False False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "print(l1_feat_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         2075      3425      3475      4825      5025      5325      6375  \\\n",
      "ID                                                                          \n",
      "329  0.096192  0.077655  0.058617  0.073146  0.150301  0.264529  0.241483   \n",
      "173  0.071889  0.212948  0.073907  0.167141  0.258142  0.444178  0.239354   \n",
      "272  0.063321  0.214897  0.070609  0.195325  0.219065  0.511882  0.237442   \n",
      "496  0.041735  0.138843  0.045350  0.083894  0.220917  0.512740  0.229494   \n",
      "182  0.166476  0.466049  0.170670  0.626819  0.319971  0.992097  0.390499   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "71   0.091350  0.437960  0.104902  0.468475  0.336644  0.712556  0.231952   \n",
      "106  0.092219  0.161396  0.073400  0.309547  0.233002  0.611231  0.355546   \n",
      "270  0.122623  0.139775  0.081102  0.139697  0.343888  0.560652  0.466123   \n",
      "435  0.150422  0.185517  0.563532  0.359765  0.457895  0.593363  0.250892   \n",
      "102  0.186658  0.277406  0.094011  0.287433  0.273583  1.000000  0.328262   \n",
      "\n",
      "         6825      9625  \n",
      "ID                       \n",
      "329  0.380261  0.302605  \n",
      "173  0.426382  0.297860  \n",
      "272  0.381608  0.321431  \n",
      "496  0.356644  0.214111  \n",
      "182  0.592040  0.545136  \n",
      "..        ...       ...  \n",
      "71   0.415092  0.501925  \n",
      "106  0.456595  0.655877  \n",
      "270  0.555452  0.461389  \n",
      "435  0.389947  0.402230  \n",
      "102  0.461578  0.332968  \n",
      "\n",
      "[412 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(new_spectrum_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the l1_best_clfs, as it takes so long to train\n",
    "\n",
    "# with open(path+'l1_best_clfs.data', 'wb') as filehandle:\n",
    "#     # store the data as binary data stream\n",
    "#     pickle.dump(l1_best_clfs, filehandle)\n",
    "    \n",
    "# with open(path+'l1_feat_list.data', 'wb') as filehandle:\n",
    "#     # store the data as binary data stream\n",
    "#     pickle.dump(l1_feat_list, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000000.0, multi_class='warn', n_jobs=None,\n",
      "                   penalty='l1', random_state=None, solver='liblinear',\n",
      "                   tol=0.0001, verbose=0, warm_start=False), LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000000.0, multi_class='warn', n_jobs=None,\n",
      "                   penalty='l1', random_state=None, solver='liblinear',\n",
      "                   tol=0.0001, verbose=0, warm_start=False), LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000000.0, multi_class='warn', n_jobs=None,\n",
      "                   penalty='l1', random_state=None, solver='liblinear',\n",
      "                   tol=0.0001, verbose=0, warm_start=False), LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000000.0, multi_class='warn', n_jobs=None,\n",
      "                   penalty='l1', random_state=None, solver='liblinear',\n",
      "                   tol=0.0001, verbose=0, warm_start=False), LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000000.0, multi_class='warn', n_jobs=None,\n",
      "                   penalty='l1', random_state=None, solver='liblinear',\n",
      "                   tol=0.0001, verbose=0, warm_start=False), LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000000.0, multi_class='warn', n_jobs=None,\n",
      "                   penalty='l1', random_state=None, solver='liblinear',\n",
      "                   tol=0.0001, verbose=0, warm_start=False), LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000000.0, multi_class='warn', n_jobs=None,\n",
      "                   penalty='l1', random_state=None, solver='liblinear',\n",
      "                   tol=0.0001, verbose=0, warm_start=False), LogisticRegression(C=10000.0, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000000.0, multi_class='warn', n_jobs=None,\n",
      "                   penalty='l1', random_state=None, solver='liblinear',\n",
      "                   tol=0.0001, verbose=0, warm_start=False), LogisticRegression(C=100.0, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000000.0, multi_class='warn', n_jobs=None,\n",
      "                   penalty='l1', random_state=None, solver='liblinear',\n",
      "                   tol=0.0001, verbose=0, warm_start=False)]\n"
     ]
    }
   ],
   "source": [
    "print(l1_best_clfs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         2075      3425      3475      4825      5025      5325      6375  \\\n",
      "ID                                                                          \n",
      "329  0.096192  0.077655  0.058617  0.073146  0.150301  0.264529  0.241483   \n",
      "173  0.071889  0.212948  0.073907  0.167141  0.258142  0.444178  0.239354   \n",
      "272  0.063321  0.214897  0.070609  0.195325  0.219065  0.511882  0.237442   \n",
      "496  0.041735  0.138843  0.045350  0.083894  0.220917  0.512740  0.229494   \n",
      "182  0.166476  0.466049  0.170670  0.626819  0.319971  0.992097  0.390499   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "71   0.091350  0.437960  0.104902  0.468475  0.336644  0.712556  0.231952   \n",
      "106  0.092219  0.161396  0.073400  0.309547  0.233002  0.611231  0.355546   \n",
      "270  0.122623  0.139775  0.081102  0.139697  0.343888  0.560652  0.466123   \n",
      "435  0.150422  0.185517  0.563532  0.359765  0.457895  0.593363  0.250892   \n",
      "102  0.186658  0.277406  0.094011  0.287433  0.273583  1.000000  0.328262   \n",
      "\n",
      "         6825      9625  \n",
      "ID                       \n",
      "329  0.380261  0.302605  \n",
      "173  0.426382  0.297860  \n",
      "272  0.381608  0.321431  \n",
      "496  0.356644  0.214111  \n",
      "182  0.592040  0.545136  \n",
      "..        ...       ...  \n",
      "71   0.415092  0.501925  \n",
      "106  0.456595  0.655877  \n",
      "270  0.555452  0.461389  \n",
      "435  0.389947  0.402230  \n",
      "102  0.461578  0.332968  \n",
      "\n",
      "[412 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# with open(path+'l1_best_clfs.data', 'rb') as filehandle:\n",
    "#     # read the data as binary data stream\n",
    "#     l1_best_clfs2 = pickle.load(filehandle)\n",
    "# with open(path+'l1_feat_list.data', 'rb') as filehandle:\n",
    "#     # store the data as binary data stream\n",
    "#     l1_feat_list2 = pickle.load(filehandle)\n",
    "\n",
    "    \n",
    "# print(apply_l1_feature_selection(spectrum_train,l1_feat_list2[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and, more often, retrieve this data\n",
    "\n",
    "with open(path+'l1_best_clfs.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    l1_best_clfs = pickle.load(filehandle)\n",
    "with open(path+'l1_feat_list.data', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    l1_feat_list = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try_clf_feat_selection\n",
    "Let's make a modification of the original try_clf in order to accept a feature_vector_list and use this in order to train classifiers with less features and more powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_clf_feat_selection(clf, params, feature_vector_list, n_cv=10):\n",
    "    t1 = time.time()\n",
    "    \n",
    "    best_classifiers = [];\n",
    "    accuracies_train = []; accuracies_test_train = [];\n",
    "    AUC_train = []; AUC_test_train = [];\n",
    "    \n",
    "    categories = targets_train.columns[:]    \n",
    "    for c,cat in enumerate(categories):\n",
    "\n",
    "        print([cat]) # indicate in which antibiotic we are\n",
    "        \n",
    "        # Selection of train and test data (depending on whether there are NaN target values)\n",
    "        X_train, Y_train = clean_nan_samples(spectrum_train,targets_train, c, cat)\n",
    "        X_test_train, Y_test_train = clean_nan_samples(spectrum_test_train,targets_test_train, c, cat)\n",
    "        \n",
    "        X_train = apply_l1_feature_selection(X_train,feature_vector_list[c])\n",
    "        X_test_train = apply_l1_feature_selection(X_test_train,feature_vector_list[c])\n",
    "            \n",
    "        # perform a GridSearchCV in order to train a classifier for this antibiotic\n",
    "        grid = GridSearchCV(clf,param_grid=params, cv=n_cv, iid=False)\n",
    "        grid.fit(X_train, Y_train)\n",
    "\n",
    "        # print the best parameters (to detect edge values), and save that classifier\n",
    "        print('The best parameters are: ',grid.best_params_)\n",
    "        best_clf = grid.best_estimator_\n",
    "        best_classifiers.append(best_clf)\n",
    "        \n",
    "        # compute the accuracy of the classifier\n",
    "        acc_train = best_clf.score(X_train, Y_train)\n",
    "        acc_test = best_clf.score(X_test_train, Y_test_train)\n",
    "        print('Train accuracy: ',np.round(acc_train,4),' and test_train accuracy: ',np.round(acc_test,4))\n",
    "        accuracies_train.append(acc_train)\n",
    "        accuracies_test_train.append(acc_test)\n",
    "        \n",
    "        # compute the AUC of the classifier\n",
    "        if callable(getattr(best_clf,\"predict_proba\",None)):\n",
    "            pred_train = best_clf.predict_proba(X_train)[:,-1] # only take last column, the prob of Y = +1\n",
    "            pred_test = best_clf.predict_proba(X_test_train)[:,-1]\n",
    "        else:\n",
    "            print('Using decision_function instead of predict_proba')\n",
    "            pred_train = best_clf.decision_function(X_train)\n",
    "            pred_test = best_clf.decision_function(X_test_train)            \n",
    "        auc_score_train = roc_auc_score(Y_train, pred_train)\n",
    "        auc_score_test = roc_auc_score(Y_test_train, pred_test)\n",
    "        print('Train AUC: ',np.round(auc_score_train,4),' and test_train AUC: ',np.round(auc_score_test,4))\n",
    "        AUC_train.append(auc_score_train)\n",
    "        AUC_test_train.append(auc_score_test)\n",
    "        \n",
    "    avg_AUC_train = np.mean(AUC_train)\n",
    "    avg_AUC_test_train = np.mean(AUC_test_train)\n",
    "    print('\\n\\nThe average train AUC is',np.round(avg_AUC_train,4),'and the avg test_train AUC is',np.round(avg_AUC_test_train,4))\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print('\\nFull execution took ',np.round(t2-t1,1),'seconds')\n",
    "    print('\\nDONE!')\n",
    "    return best_classifiers, accuracies_train, accuracies_test_train, AUC_train, AUC_test_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OXACILINA']\n",
      "The best parameters are:  {'C': 10000.0, 'gamma': 0.1}\n",
      "Train accuracy:  0.9636  and test_train accuracy:  0.7981\n",
      "Using decision_function instead of predict_proba\n",
      "Train AUC:  0.9872  and test_train AUC:  0.8687\n",
      "['AMIKACINA']\n",
      "The best parameters are:  {'C': 10000.0, 'gamma': 0.01}\n",
      "Train accuracy:  0.8613  and test_train accuracy:  0.7805\n",
      "Using decision_function instead of predict_proba\n",
      "Train AUC:  0.9415  and test_train AUC:  0.7294\n",
      "['AMOXI/CLAV']\n",
      "The best parameters are:  {'C': 10000.0, 'gamma': 0.1}\n",
      "Train accuracy:  0.9587  and test_train accuracy:  0.7692\n",
      "Using decision_function instead of predict_proba\n",
      "Train AUC:  0.9886  and test_train AUC:  0.8562\n",
      "['CIPROFLOXACINO']\n",
      "The best parameters are:  {'C': 100000.0, 'gamma': 0.01}\n",
      "Train accuracy:  0.9053  and test_train accuracy:  0.7788\n",
      "Using decision_function instead of predict_proba\n",
      "Train AUC:  0.9573  and test_train AUC:  0.7867\n",
      "['CLINDAMICINA']\n",
      "The best parameters are:  {'C': 1.0, 'gamma': 1000.0}\n",
      "Train accuracy:  0.9903  and test_train accuracy:  0.75\n",
      "Using decision_function instead of predict_proba\n",
      "Train AUC:  0.9964  and test_train AUC:  0.5024\n",
      "['ERITROMICINA']\n",
      "The best parameters are:  {'C': 100000.0, 'gamma': 0.01}\n",
      "Train accuracy:  0.8859  and test_train accuracy:  0.7019\n",
      "Using decision_function instead of predict_proba\n",
      "Train AUC:  0.9465  and test_train AUC:  0.7307\n",
      "['LEVOFLOXACINO']\n",
      "The best parameters are:  {'C': 100000.0, 'gamma': 0.01}\n",
      "Train accuracy:  0.9293  and test_train accuracy:  0.7353\n",
      "Using decision_function instead of predict_proba\n",
      "Train AUC:  0.9737  and test_train AUC:  0.7766\n",
      "['PENICILINA']\n",
      "The best parameters are:  {'C': 1.0, 'gamma': 100.0}\n",
      "Train accuracy:  1.0  and test_train accuracy:  0.9231\n",
      "Using decision_function instead of predict_proba\n",
      "Train AUC:  1.0  and test_train AUC:  0.5745\n",
      "['TOBRAMICINA']\n",
      "The best parameters are:  {'C': 10000.0, 'gamma': 0.01}\n",
      "Train accuracy:  0.8632  and test_train accuracy:  0.7831\n",
      "Using decision_function instead of predict_proba\n",
      "Train AUC:  0.9383  and test_train AUC:  0.7837\n",
      "\n",
      "\n",
      "The average train AUC is 0.9699 and the avg test_train AUC is 0.7343\n",
      "\n",
      "Full execution took  154.7 seconds\n",
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='rbf', class_weight='balanced')\n",
    "\n",
    "C_vector = 10. ** np.arange(-4,6)\n",
    "gamma_vector = 10. ** np.arange(-4,4)\n",
    "params = {'C':C_vector, 'gamma':gamma_vector}\n",
    "\n",
    "rbf_SVM_best_clfs, _, _, rbf_SVM_AUC_train, rbf_SVM_AUC_test_train = try_clf_feat_selection(clf,params,l1_feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OXACILINA']\n",
      "The best parameters are:  {'max_depth': 9, 'n_estimators': 50}\n",
      "Train accuracy:  0.9951  and test_train accuracy:  0.7692\n",
      "Train AUC:  1.0  and test_train AUC:  0.796\n",
      "['AMIKACINA']\n",
      "The best parameters are:  {'max_depth': 9, 'n_estimators': 50}\n",
      "Train accuracy:  0.9884  and test_train accuracy:  0.7683\n",
      "Train AUC:  0.9991  and test_train AUC:  0.7052\n",
      "['AMOXI/CLAV']\n",
      "The best parameters are:  {'max_depth': 9, 'n_estimators': 20}\n",
      "Train accuracy:  0.9854  and test_train accuracy:  0.7212\n",
      "Train AUC:  0.9994  and test_train AUC:  0.7241\n",
      "['CIPROFLOXACINO']\n",
      "The best parameters are:  {'max_depth': 6, 'n_estimators': 30}\n",
      "Train accuracy:  0.9733  and test_train accuracy:  0.7115\n",
      "Train AUC:  0.9975  and test_train AUC:  0.6877\n",
      "['CLINDAMICINA']\n",
      "The best parameters are:  {'max_depth': 9, 'n_estimators': 50}\n",
      "Train accuracy:  0.9879  and test_train accuracy:  0.7596\n",
      "Train AUC:  0.9975  and test_train AUC:  0.5574\n",
      "['ERITROMICINA']\n",
      "The best parameters are:  {'max_depth': 8, 'n_estimators': 30}\n",
      "Train accuracy:  0.9806  and test_train accuracy:  0.6827\n",
      "Train AUC:  0.9989  and test_train AUC:  0.7045\n",
      "['LEVOFLOXACINO']\n",
      "The best parameters are:  {'max_depth': 8, 'n_estimators': 20}\n",
      "Train accuracy:  0.9829  and test_train accuracy:  0.6765\n",
      "Train AUC:  0.9983  and test_train AUC:  0.6761\n",
      "['PENICILINA']\n",
      "The best parameters are:  {'max_depth': 7, 'n_estimators': 70}\n",
      "Train accuracy:  1.0  and test_train accuracy:  0.9038\n",
      "Train AUC:  1.0  and test_train AUC:  0.7936\n",
      "['TOBRAMICINA']\n",
      "The best parameters are:  {'max_depth': 8, 'n_estimators': 30}\n",
      "Train accuracy:  0.9829  and test_train accuracy:  0.759\n",
      "Train AUC:  0.9992  and test_train AUC:  0.6931\n",
      "\n",
      "\n",
      "The average train AUC is 0.9989 and the avg test_train AUC is 0.7042\n",
      "\n",
      "Full execution took  153.5 seconds\n",
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(class_weight='balanced',min_samples_split=5)\n",
    "params = {'n_estimators':[10,20,30,50,70],'max_depth':np.arange(1,10)}\n",
    "# later try\n",
    "# params = {'n_estimators':[10,20,30,50,70,100,150],'max_depth':np.arange(1,30)}\n",
    "rf_best_clfs, _, _, rf_AUC_train, rf_AUC_test_train = try_clf_feat_selection(clf,params,l1_feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OXACILINA']\n",
      "The best parameters are:  {'n_estimators': 150}\n",
      "Train accuracy:  0.9951  and test_train accuracy:  0.6827\n",
      "Train AUC:  1.0  and test_train AUC:  0.7351\n",
      "['AMIKACINA']\n",
      "The best parameters are:  {'n_estimators': 150}\n",
      "Train accuracy:  0.9884  and test_train accuracy:  0.7439\n",
      "Train AUC:  0.9997  and test_train AUC:  0.7327\n",
      "['AMOXI/CLAV']\n",
      "The best parameters are:  {'n_estimators': 150}\n",
      "Train accuracy:  0.9951  and test_train accuracy:  0.7019\n",
      "Train AUC:  0.9999  and test_train AUC:  0.7336\n",
      "['CIPROFLOXACINO']\n",
      "The best parameters are:  {'n_estimators': 150}\n",
      "Train accuracy:  0.9879  and test_train accuracy:  0.7212\n",
      "Train AUC:  0.9997  and test_train AUC:  0.7133\n",
      "['CLINDAMICINA']\n",
      "The best parameters are:  {'n_estimators': 150}\n",
      "Train accuracy:  0.949  and test_train accuracy:  0.7308\n",
      "Train AUC:  0.9933  and test_train AUC:  0.5891\n",
      "['ERITROMICINA']\n",
      "The best parameters are:  {'n_estimators': 150}\n",
      "Train accuracy:  0.9951  and test_train accuracy:  0.625\n",
      "Train AUC:  0.9999  and test_train AUC:  0.615\n",
      "['LEVOFLOXACINO']\n",
      "The best parameters are:  {'n_estimators': 150}\n",
      "Train accuracy:  0.9902  and test_train accuracy:  0.7647\n",
      "Train AUC:  0.9998  and test_train AUC:  0.7444\n",
      "['PENICILINA']\n",
      "The best parameters are:  {'n_estimators': 150}\n",
      "Train accuracy:  1.0  and test_train accuracy:  0.9135\n",
      "Train AUC:  1.0  and test_train AUC:  0.767\n",
      "['TOBRAMICINA']\n",
      "The best parameters are:  {'n_estimators': 150}\n",
      "Train accuracy:  0.9886  and test_train accuracy:  0.747\n",
      "Train AUC:  0.9997  and test_train AUC:  0.767\n",
      "\n",
      "\n",
      "The average train AUC is 0.9991 and the avg test_train AUC is 0.7108\n",
      "\n",
      "Full execution took  43.8 seconds\n",
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1)) # we can change that\n",
    "params = {'n_estimators':[150]}\n",
    "ab_best_clfs, _, _, ab_AUC_train, ab_AUC_test_train = try_clf_feat_selection(clf,params,l1_feat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
