{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import _pickle\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import time\n",
    "import os\n",
    "\n",
    "from ourfunctions_v3 import clean_nan_samples, try_clf, apply_feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2000; M = 12000; \n",
    "step_size = 1; # interpolation step size\n",
    "\n",
    "path = \"D:/GitHub/Machine-Learning/Kaggle/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = path + 'Kaggle classifiers/bin size 5/'\n",
    "path_results = path + 'Kaggle_results/'\n",
    "\n",
    "ncpu = os.cpu_count()\n",
    "if (ncpu>2): njobs = ncpu - 2; \n",
    "else: njobs = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile(path+'zipped_TrainData.zip', 'r')\n",
    "df_train = _pickle.loads(zf.open('TrainData.pkl').read())\n",
    "zf.close()\n",
    "\n",
    "zf = zipfile.ZipFile(path+'zipped_TestDataUnlabeled.zip', 'r')\n",
    "df_test = _pickle.loads(zf.open('TestDataUnlabeled.pkl').read())\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(df):\n",
    "    N = len(df)  # number of samples\n",
    "    idx_list = []\n",
    "    for idx in range(N): \n",
    "        intensity = df[['intensity']].iloc[idx].values[0]\n",
    "        mzcoord   = df[['coord_mz']].iloc[idx].values[0]\n",
    "\n",
    "        if np.var(intensity) < 100:\n",
    "          idx_list.append(idx)\n",
    "          print('Training sample', idx, ' eliminated')\n",
    "    new_df = df.drop(index = idx_list)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample 339  eliminated\n",
      "Training sample 490  eliminated\n"
     ]
    }
   ],
   "source": [
    "df_train = remove_noise(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_spectra(df, m, M, step_size):\n",
    "    # step_size is the size of each step; 1 interpolates very well.\n",
    "    mz_range = np.arange(m,M+1,step_size)\n",
    "    \n",
    "    N = len(df)  # number of samples\n",
    "    L = len(mz_range)  # length of new spectrum (number of bins)\n",
    "    all_data = np.zeros((N,L))\n",
    "    idx_list = []\n",
    "    \n",
    "    for idx in range(N): \n",
    "        intensity = df[['intensity']].iloc[idx].values[0]       \n",
    "        mzcoord   = df[['coord_mz']].iloc[idx].values[0]\n",
    "        interpolated_spectrum = np.interp(x=mz_range,xp=mzcoord,fp=intensity)\n",
    "        interpolated_spectrum = interpolated_spectrum / np.max(interpolated_spectrum)\n",
    "        all_data[idx,:] = interpolated_spectrum\n",
    "    new_df = pd.DataFrame(data=all_data, columns = mz_range, index = df.index)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 359 and test_train samples: 155\n",
      "Spectrum interpolated!\n"
     ]
    }
   ],
   "source": [
    "# df_train = df_train.drop_duplicates(subset='ID_sample') # eliminate duplicates\n",
    "# Let's work without duplicates from now on, to avoid having same sample in training and test_training sets\n",
    "\n",
    "# Extract data (spectra) and targets of the df_train set\n",
    "data = df_train.iloc[:,-2:]\n",
    "targets = df_train.iloc[:,1:-2]\n",
    "\n",
    "# Then, split into a train and test_train set\n",
    "data_train, data_test_train, targets_train, targets_test_train = train_test_split(data, targets, test_size=0.3, random_state=0) # split the data\n",
    "print('Training samples: '+str(len(data_train))+' and test_train samples: ' + str(len(data_test_train)) )\n",
    "\n",
    "# apply the bins to all spectra, so that our feature space becomes the same for all samples (make them regular, all the same)\n",
    "spectrum_train = interpolate_spectra(data_train,m,M,step_size)\n",
    "spectrum_test_train = interpolate_spectra(data_test_train,m,M,step_size)\n",
    "print('Spectrum interpolated!')\n",
    "# these spectrum_... are our X for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
