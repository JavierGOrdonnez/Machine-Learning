{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import _pickle\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing\n",
    "Remember to change the path if needed be\n",
    "\n",
    "### Hyperparameters of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2000; M = 12000; \n",
    "bin_size = 5;\n",
    "\n",
    "# path = \"D:/GitHub/Machine-Learning/Kaggle/\"\n",
    "path = \"C:/Users/Javi/Documents/GitHub/Machine-Learning/Kaggle/\"\n",
    "\n",
    "# # Take the data from Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# path = \"/content/drive/My Drive/Colab Notebooks/Kaggle/Kaggle_data/\"\n",
    "# path_results = \"/content/drive/My Drive/Colab Notebooks/Kaggle/Kaggle_results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile(path+'zipped_TrainData.zip', 'r')\n",
    "df_train = _pickle.loads(zf.open('TrainData.pkl').read())\n",
    "zf.close()\n",
    "\n",
    "zf = zipfile.ZipFile(path+'zipped_TestDataUnlabeled.zip', 'r')\n",
    "df_test = _pickle.loads(zf.open('TestDataUnlabeled.pkl').read())\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spectrum_in_bins(df,m,M,bin_size):\n",
    "    # Now, let's define the mz ranges, and the label associated to each of them (the mean of the limiting values of each bin)\n",
    "    range_min = []; range_max = []; range_label = [];\n",
    "    for mz in range(m,M,bin_size):\n",
    "        range_min.append(mz)\n",
    "        range_max.append(mz+bin_size)\n",
    "        range_label.append(np.mean([range_min[-1],range_max[-1]]).astype(int))\n",
    "    N = len(df)  # number of samples\n",
    "    L = len(range_min)  # length of new spectrum (number of bins)\n",
    "    all_data = np.zeros((N,L))\n",
    "    for idx in range(N): \n",
    "        intensity = df[['intensity']].iloc[idx].values[0]\n",
    "        mzcoord   = df[['coord_mz']].iloc[idx].values[0]\n",
    "        idx_data_in_bins = np.zeros((1,L))\n",
    "        for i,mz in enumerate(range_min):\n",
    "            intensity_range = intensity[(mzcoord > mz) & (mzcoord < (mz+bin_size))]\n",
    "            if len(intensity_range) > 0 :\n",
    "                idx_data_in_bins[0,i] = np.max(intensity_range)\n",
    "            else: # if those mz coordinates are not in that spectrum\n",
    "                idx_data_in_bins[0,i] = 0   \n",
    "\n",
    "        # Normalize the amplitude of the spectrum\n",
    "        idx_data_in_bins[0,:] = idx_data_in_bins[0,:] / np.max(idx_data_in_bins[0,:])\n",
    "        all_data[idx,:] = idx_data_in_bins\n",
    "    new_df = pd.DataFrame(data=all_data, columns = range_label, index = df.index)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 361 and test_train samples: 155\n",
      "Spectrum regularized!\n"
     ]
    }
   ],
   "source": [
    "# Extract data (spectra) and targets of the df_train set\n",
    "data = df_train.iloc[:,-2:]\n",
    "targets = df_train.iloc[:,1:-2]\n",
    "\n",
    "# Then, split into a train and test_train set\n",
    "data_train, data_test_train, targets_train, targets_test_train = train_test_split(data, targets, test_size=0.3, random_state=25) # split the data\n",
    "print('Training samples: '+str(len(data_train))+' and test_train samples: ' + str(len(data_test_train)) )\n",
    "\n",
    "# apply the bins to all spectra, so that our feature space becomes the same for all samples (make them regular, all the same)\n",
    "spectrum_train = spectrum_in_bins(data_train,m,M,bin_size)\n",
    "spectrum_test_train = spectrum_in_bins(data_test_train,m,M,bin_size)\n",
    "print('Spectrum regularized!')\n",
    "# these spectrum_... are our X for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nan_samples(spectrum,targets, c, cat):\n",
    "# if there are any NaN values, we should remove those samples\n",
    "    if (targets[cat].isnull().sum() > 0).all(): \n",
    "        merged = pd.concat([spectrum , targets],axis=1,copy=True)\n",
    "        clean = merged.dropna(subset=[cat])\n",
    "        print('Dropped ',len(merged)-len(clean))\n",
    "        Y = clean.iloc[:,-9+c].to_numpy().reshape(-1,)\n",
    "        X = clean.iloc[:,:-9]\n",
    "\n",
    "    else:\n",
    "        Y = targets.iloc[:,c].to_numpy().reshape(-1,)\n",
    "        X = spectrum.copy(deep=True)\n",
    "    return X , Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different classifiers\n",
    "The try_clf function has been built for, given a classifier and a parameter dictionary (for hyperparameter cross-validation), create a classifier for each antibiotic, and return the results. This enables for fast testing of different classifiers. Moreover, the function also takes charge of suppressing NaN values in the targets ocurring for amikacina, levofloxacino and tobramicina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def try_clf(clf,params,n_cv=10):\n",
    "    t1 = time.time()\n",
    "    \n",
    "    best_classifiers = [];\n",
    "    accuracies_train = []; accuracies_test_train = [];\n",
    "    AUC_train = []; AUC_test_train = [];\n",
    "    \n",
    "    categories = targets_train.columns[:]    \n",
    "    for c,cat in enumerate(categories):\n",
    "\n",
    "        print([cat]) # indicate in which antibiotic we are\n",
    "        \n",
    "        # Selection of train and test data (depending on whether there are NaN target values)\n",
    "        X_train, Y_train = clean_nan_samples(spectrum_train,targets_train, c, cat)\n",
    "        X_test_train, Y_test_train = clean_nan_samples(spectrum_test_train,targets_test_train, c, cat)\n",
    "            \n",
    "        # perform a GridSearchCV in order to train a classifier for this antibiotic\n",
    "        grid = GridSearchCV(clf,param_grid=params, cv=n_cv, iid=False)\n",
    "        grid.fit(X_train, Y_train)\n",
    "\n",
    "        # print the best parameters (to detect edge values), and save that classifier\n",
    "        print('The best parameters are: ',grid.best_params_)\n",
    "        best_clf = grid.best_estimator_\n",
    "        best_classifiers.append(best_clf)\n",
    "        \n",
    "        # compute the accuracy of the classifier\n",
    "        acc_train = best_clf.score(X_train, Y_train)\n",
    "        acc_test = best_clf.score(X_test_train, Y_test_train)\n",
    "        print('Train accuracy: ',np.round(acc_train,4),' and test_train accuracy: ',np.round(acc_test,4))\n",
    "        accuracies_train.append(acc_train)\n",
    "        accuracies_test_train.append(acc_test)\n",
    "        \n",
    "        # compute the AUC of the classifier\n",
    "        if callable(getattr(best_clf,\"predict_proba\",None)):\n",
    "            pred_train = best_clf.predict_proba(X_train)[:,-1] # only take last column, the prob of Y = +1\n",
    "            pred_test = best_clf.predict_proba(X_test_train)[:,-1]\n",
    "        else:\n",
    "            print('Using decision_function instead of predict_proba')\n",
    "            pred_train = best_clf.decision_function(X_train)\n",
    "            pred_test = best_clf.decision_function(X_test_train)            \n",
    "        auc_score_train = roc_auc_score(Y_train, pred_train)\n",
    "        auc_score_test = roc_auc_score(Y_test_train, pred_test)\n",
    "        print('Train AUC: ',np.round(auc_score_train,4),' and test_train AUC: ',np.round(auc_score_test,4))\n",
    "        AUC_train.append(auc_score_train)\n",
    "        AUC_test_train.append(auc_score_test)\n",
    "        \n",
    "    avg_AUC_train = np.mean(AUC_train)\n",
    "    avg_AUC_test_train = np.mean(AUC_test_train)\n",
    "    print('\\n\\nThe average train AUC is',np.round(avg_AUC_train,4),'and the avg test_train AUC is',np.round(avg_AUC_test_train,4))\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print('\\nFull execution took ',np.round(t2-t1,1),'seconds')\n",
    "    print('\\nDONE!')\n",
    "    return best_classifiers, accuracies_train, accuracies_test_train, AUC_train, AUC_test_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also enable L1 feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l1_clfs():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(penalty='l1',solver='liblinear',max_iter=1e6, class_weight='balanced')\n",
    "    params = {'C':10.**np.arange(-2,5)}\n",
    "    l1_best_clfs,_,_,_,_ = try_clf(clf,params)\n",
    "    return l1_best_clfs\n",
    "\n",
    "def obtain_l1_vects(l1_best_clfs,spectrum_train,targets_train):\n",
    "    l1_feat_list = []    \n",
    "    categories = targets_train.columns[:]\n",
    "    \n",
    "    for c, cat in enumerate(categories):\n",
    "        n = np.sum(np.abs(l1_best_clfs[c].coef_) > 0)\n",
    "        print('Number of features:',n)\n",
    "        while n == 0:\n",
    "            clf = l1_best_clfs[c]\n",
    "            c_value = clf.get_params()['C']\n",
    "            new_c = c_value * 10\n",
    "            clf.set_params(C=new_c)\n",
    "            X_train, Y_train = clean_nan_samples(spectrum_train,targets_train, c, cat)            \n",
    "            clf.fit(X_train, Y_train) # refit with higher C\n",
    "            l1_best_clfs[c] = clf\n",
    "            n = np.sum(np.abs(clf.coef_) > 0)\n",
    "            print(n)\n",
    "    \n",
    "        # once we know we have at least one non-zero feature\n",
    "        vect = (np.abs(l1_best_clfs[c].coef_)>0).reshape(-1,)\n",
    "        l1_feat_list.append(vect)\n",
    "    return l1_feat_list\n",
    "\n",
    "# to be applyied to each category\n",
    "def apply_l1_feature_selection(spectrum_train,vect): # vect is l1_feat_list[c]\n",
    "    new_spectrum = spectrum_train.copy(deep=True).iloc[:,vect]   \n",
    "    return new_spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_clf_feat_selection(clf, params, feature_vector_list, n_cv=10):\n",
    "    t1 = time.time()\n",
    "    \n",
    "    best_classifiers = [];\n",
    "    accuracies_train = []; accuracies_test_train = [];\n",
    "    AUC_train = []; AUC_test_train = [];\n",
    "    \n",
    "    categories = targets_train.columns[:]    \n",
    "    for c,cat in enumerate(categories):\n",
    "\n",
    "        print([cat]) # indicate in which antibiotic we are\n",
    "        \n",
    "        # Selection of train and test data (depending on whether there are NaN target values)\n",
    "        X_train, Y_train = clean_nan_samples(spectrum_train,targets_train, c, cat)\n",
    "        X_test_train, Y_test_train = clean_nan_samples(spectrum_test_train,targets_test_train, c, cat)\n",
    "        \n",
    "        X_train = apply_l1_feature_selection(X_train,feature_vector_list[c])\n",
    "        X_test_train = apply_l1_feature_selection(X_test_train,feature_vector_list[c])\n",
    "            \n",
    "        # perform a GridSearchCV in order to train a classifier for this antibiotic\n",
    "        grid = GridSearchCV(clf,param_grid=params, cv=n_cv, iid=False)\n",
    "        grid.fit(X_train, Y_train)\n",
    "\n",
    "        # print the best parameters (to detect edge values), and save that classifier\n",
    "        print('The best parameters are: ',grid.best_params_)\n",
    "        best_clf = grid.best_estimator_\n",
    "        best_classifiers.append(best_clf)\n",
    "        \n",
    "        # compute the accuracy of the classifier\n",
    "        acc_train = best_clf.score(X_train, Y_train)\n",
    "        acc_test = best_clf.score(X_test_train, Y_test_train)\n",
    "        print('Train accuracy: ',np.round(acc_train,4),' and test_train accuracy: ',np.round(acc_test,4))\n",
    "        accuracies_train.append(acc_train)\n",
    "        accuracies_test_train.append(acc_test)\n",
    "        \n",
    "        # compute the AUC of the classifier\n",
    "        if callable(getattr(best_clf,\"predict_proba\",None)):\n",
    "            pred_train = best_clf.predict_proba(X_train)[:,-1] # only take last column, the prob of Y = +1\n",
    "            pred_test = best_clf.predict_proba(X_test_train)[:,-1]\n",
    "        else:\n",
    "            print('Using decision_function instead of predict_proba')\n",
    "            pred_train = best_clf.decision_function(X_train)\n",
    "            pred_test = best_clf.decision_function(X_test_train)            \n",
    "        auc_score_train = roc_auc_score(Y_train, pred_train)\n",
    "        auc_score_test = roc_auc_score(Y_test_train, pred_test)\n",
    "        print('Train AUC: ',np.round(auc_score_train,4),' and test_train AUC: ',np.round(auc_score_test,4))\n",
    "        AUC_train.append(auc_score_train)\n",
    "        AUC_test_train.append(auc_score_test)\n",
    "        \n",
    "    avg_AUC_train = np.mean(AUC_train)\n",
    "    avg_AUC_test_train = np.mean(AUC_test_train)\n",
    "    print('\\n\\nThe average train AUC is',np.round(avg_AUC_train,4),'and the avg test_train AUC is',np.round(avg_AUC_test_train,4))\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print('\\nFull execution took ',np.round(t2-t1,1),'seconds')\n",
    "    print('\\nDONE!')\n",
    "    return best_classifiers, accuracies_train, accuracies_test_train, AUC_train, AUC_test_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Javi\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.21.3 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "with open(path+'l1_best_clfs.data', 'rb') as filehandle:\n",
    "    # read the data as binary data stream\n",
    "    l1_best_clfs = pickle.load(filehandle)\n",
    "with open(path+'l1_feat_list.data', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    l1_feat_list = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real AdaBoost with HW2 method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "def RAEnsemble(X_train, Y_train, X_test, Y_test, T):\n",
    "  '''\n",
    "  Train and evaluate a bagged ensemble of decission trees\n",
    "\n",
    "    Args:\n",
    "        X_train(numpy dnarray): training (number training data x number dimensions). \n",
    "        Y_train (numpy dnarray): labels of the training data (number training data x 1).\n",
    "        X_test(numpy dnarray): test data to evaluate the ensemble performance (number test data x number dimensions).\n",
    "        Y_test (numpy dnarray): labels of the test data (number test data x 1).\n",
    "        T: number of learners in the ensemble                                        \n",
    "   Returns:\n",
    "        acc_tree_train (numpy dnarray): accuracy over the training data for different number of learners. \n",
    "                It's a vector length T, where the t-th element is the ensemble accuracy when only t trees are used.\n",
    "        acc_tree_test (numpy dnarray): accuracy over the test data for different number of learners. \n",
    "                It's a vector length T, where the t-th element is the ensemble accuracy when only t trees are used.\n",
    "        Y_pred_train (numpy dnarray): predicted outputs over the training data. It's a matrix of dimensions T x number of training samples, \n",
    "          where the t-th row has the predicted outputs when t trees are used.\n",
    "        Y_pred_test (numpy dnarray): predicted outputs over the test data. It's a matrix of dimensions T x number of test samples, \n",
    "          where the t-th row has the predicted outputs when t trees are used.\n",
    "        alpha (numpy dnarray): weight vector of length T with the ouput weights assigned to combine the learner's outputs.\n",
    "        Dt_all (numpy dnarray): emphasis function used to train each learner. It's a matrix of dimensions T x number of training samples, \n",
    "          where the t-th row has the emphasis function used by the t-th learner.\n",
    "\n",
    "  '''  \n",
    "  # <SOL>\n",
    "  acc_tree_train = np.zeros((T,))\n",
    "  acc_tree_test = np.zeros((T,))    \n",
    "\n",
    "  auc_tree_train = np.zeros((T,))\n",
    "  auc_tree_test = np.zeros((T,))\n",
    "    \n",
    "  auc_ensemble_train = np.zeros((T,))\n",
    "  auc_ensemble_test = np.zeros((T,))\n",
    "\n",
    "  Y_pred_train = np.zeros((T,Y_train.shape[0]))\n",
    "  Y_pred_test  = np.zeros((T,Y_test.shape[0]))\n",
    "\n",
    "  alpha = np.zeros((T,1))\n",
    "  Dt_all = np.zeros((T,X_train.shape[0]))\n",
    "\n",
    "  outputs_train = np.zeros((T,Y_train.shape[0]))\n",
    "  outputs_test  = np.zeros((T,Y_test.shape[0]))\n",
    "\n",
    "  f_train = np.zeros((T,Y_train.shape[0]))\n",
    "  f_test  = np.zeros((T,Y_test.shape[0]))\n",
    "\n",
    "  Dt_all[0,:] = np.ones((1,X_train.shape[0])) / X_train.shape[0]  # Initialize all weights as 1 / n_samples\n",
    "  for i in range(T):\n",
    "    mytree = DecisionTreeClassifier(max_depth=2)\n",
    "    mytree.fit(X_train,Y_train,sample_weight=Dt_all[i,:])\n",
    "\n",
    "    # For real-valued predictions:\n",
    "    outputs_train[i,:] = np.dot(mytree.predict_proba(X_train),mytree.classes_)\n",
    "    outputs_test[i,:]  = np.dot(mytree.predict_proba(X_test) ,mytree.classes_)\n",
    "#     print(mytree.classes_)\n",
    "    # Get individual tree AUC\n",
    "    auc_tree_train[i] = roc_auc_score(Y_train, outputs_train[i,:])\n",
    "    auc_tree_test[i] = roc_auc_score(Y_test, outputs_test[i,:])\n",
    "\n",
    "    # Get gamma and alpha_i values\n",
    "    gamma = np.dot(Dt_all[i,:],np.multiply(outputs_train[i,:],Y_train))\n",
    "    alpha[i] = 0.5 * np.log((1+gamma)/(1-gamma))\n",
    "\n",
    "    # Update emphasis function (except if in the last iteration)\n",
    "    if (i < T-1):\n",
    "      emphasis = np.multiply(Dt_all[i,:],np.exp(-1*alpha[i]*np.multiply(outputs_train[i,:],Y_train)))\n",
    "      Dt_all[i+1,:] = emphasis / np.sum(emphasis) # normalize\n",
    "\n",
    "    # Now let's obtain the f values (outputs of each tree * alphas)\n",
    "    f_train[i,:] = np.sum(np.multiply(outputs_train[0:i+1,:],alpha[0:i+1]) , axis=0) #/ np.sum(alpha[0:i+1])\n",
    "    f_test[i,:]  = np.sum(np.multiply(outputs_test[0:i+1,:],alpha[0:i+1])  , axis=0) #/ np.sum(alpha[0:i+1])\n",
    "\n",
    "    # Depending on whether f_i is greater or smaller than 0, set the label to +1 or -1\n",
    "    predicted_labels_train = copy.deepcopy(f_train[i,:])\n",
    "    predicted_labels_train[predicted_labels_train >=0] = 1\n",
    "    predicted_labels_train[predicted_labels_train < 0] = -1\n",
    "    Y_pred_train[i,:] = predicted_labels_train\n",
    "    acc_tree_train[i] = acc(Y_train,Y_pred_train[i,:])\n",
    "\n",
    "    predicted_labels_test = copy.deepcopy(f_test[i,:])\n",
    "    predicted_labels_test[predicted_labels_test >=0] = 1\n",
    "    predicted_labels_test[predicted_labels_test < 0] = -1\n",
    "    Y_pred_test[i,:] = predicted_labels_test\n",
    "    acc_tree_test[i] = acc(Y_test,Y_pred_test[i,:])\n",
    "    \n",
    "    auc_ensemble_train[i] = roc_auc_score(Y_train,f_train[i,:])\n",
    "    auc_ensemble_test[i] = roc_auc_score(Y_test,f_test[i,:])\n",
    "\n",
    "\n",
    "  # </SOL>\n",
    "  return Y_pred_train, Y_pred_test, acc_tree_train, acc_tree_test,  alpha, Dt_all, f_train, f_test, auc_tree_train, auc_tree_test, auc_ensemble_train, auc_ensemble_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = targets_train.columns[:]\n",
    "c = 0; cat = categories[c];\n",
    "\n",
    "X_train, Y_train = clean_nan_samples(spectrum_train,targets_train, c, cat)\n",
    "X_test_train, Y_test_train = clean_nan_samples(spectrum_test_train,targets_test_train, c, cat)\n",
    "Y_train[Y_train==0] = -1\n",
    "Y_test_train[Y_test_train==0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "Y_pred_train, Y_pred_test, acc_tree_train, acc_tree_test,  alpha, Dt_all, f_train, f_test, auc_tree_train, auc_tree_test, auc_ensemble_train, auc_ensemble_test = RAEnsemble(X_train, Y_train, X_test_train, Y_test_train, T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Evolution of accuracy with the number of trees used\n",
    " plt.figure()\n",
    " plt.plot(acc_tree_train,label='Training accuracy')\n",
    " plt.plot(acc_tree_test,label='Test accuracy')\n",
    " plt.title('Evolution of accuracy with number of trees used')\n",
    " plt.xlabel('Number of trees used')\n",
    " plt.ylabel('Accuracy')\n",
    " plt.legend()\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plt.figure()\n",
    " plt.plot(auc_ensemble_train,label='Training AUC')\n",
    " plt.plot(auc_ensemble_test,label='Test AUC')\n",
    " plt.title('Evolution of AUC with number of trees used')\n",
    " plt.xlabel('Number of trees used')\n",
    " plt.ylabel('Accuracy')\n",
    " plt.legend()\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_curve + auc methods (same, but also enables plotting)\n",
    "fpr, tpr, _ = roc_curve(Y_test_train,f_test[-1,:])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print ('ROC_curve + ROC_AUC method: \\t',roc_auc )\n",
    "\n",
    "plt.plot(fpr, tpr, label='ROC curve of class')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve per class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLINDAMICINA\n",
    "The 4th antibiotic, clindamicina, is one of the hardest to classify (most methods yield AUC = 0.6 or below). Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = targets_train.columns[:]\n",
    "c = 4; cat = categories[c];\n",
    "# this antibiotic is one of the hardest to classify\n",
    "\n",
    "X_train, Y_train = clean_nan_samples(spectrum_train,targets_train, c, cat)\n",
    "X_test_train, Y_test_train = clean_nan_samples(spectrum_test_train,targets_test_train, c, cat)\n",
    "Y_train[Y_train==0] = -1\n",
    "Y_test_train[Y_test_train==0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "Y_pred_train, Y_pred_test, acc_tree_train, acc_tree_test,  alpha, Dt_all, f_train, f_test, auc_tree_train, auc_tree_test, auc_ensemble_train, auc_ensemble_test = RAEnsemble(X_train, Y_train, X_test_train, Y_test_train, T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " plt.figure()\n",
    " plt.plot(auc_ensemble_train,label='Training AUC')\n",
    " plt.plot(auc_ensemble_test,label='Test AUC')\n",
    " plt.title('Evolution of AUC with number of trees used')\n",
    " plt.xlabel('Number of trees used')\n",
    " plt.ylabel('Accuracy')\n",
    " plt.legend()\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are not very good, but actually somehow better than those of other methods (LogReg, RF...). Remarkably, they are substantially better than those of AdaBoostClassifier of sklearn (with 200 estimators, also depth 2, it yields 0.5644)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaBoost for all categories, and getting Y_test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate final results in CSV format\n",
    "From \"Jagger_4_GenerateCSV_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_spectre(spectre, df):\n",
    "  # Include the ID_sample column for the group_by\n",
    "  spectre['ID_sample'] = df.ID_sample\n",
    "  # MEAN OF THE SPECTRE\n",
    "  spectre = spectre.groupby('ID_sample').mean().reset_index()\n",
    "  return spectre\n",
    "\n",
    "def generate_csv_from_clf(clf_list, path, path_results, file_name):\n",
    "  # classifiers must be provided with parameters, and in a list [clf_antibiotic0, clf_antibiotic1, ...]\n",
    "  # spectrum and targets full train (containing all training points) will be used for training the clfs in clf list\n",
    "  # df_test must be provided as loaded from the file\n",
    "\n",
    "  # read all data from files\n",
    "  zf = zipfile.ZipFile(path+'zipped_TrainData.zip', 'r')\n",
    "  df_full_train = _pickle.loads(zf.open('TrainData.pkl').read());   zf.close()\n",
    "\n",
    "  zf = zipfile.ZipFile(path+'zipped_TestDataUnlabeled.zip', 'r')\n",
    "  df_test = _pickle.loads(zf.open('TestDataUnlabeled.pkl').read());   zf.close()\n",
    "\n",
    "  # Process test df to get UNIQUE samples and convert to spectrum\n",
    "\n",
    "  # df_unique_test = df_test.drop_duplicates(subset='ID_sample')\n",
    "\n",
    "  spectrum_test_forcsv = spectrum_in_bins(df_test,m,M,bin_size)\n",
    "  spectrum_test_forcsv = get_unique_spectre(spectrum_test_forcsv, df_test)\n",
    "\n",
    "  # Process train set to later train the clfs\n",
    "  spectrum_full_train = spectrum_in_bins(df_full_train.iloc[:,-2:],m,M,bin_size)\n",
    "  targets_full_train  = df_full_train.iloc[:,1:-2]  \n",
    "\n",
    "  # read the submission example file\n",
    "  df_submission = pd.read_csv(path+'SubmissionSample.csv') \n",
    "  categories = df_submission.columns[1:]\n",
    "  df_submission['ID']= spectrum_test_forcsv['ID_sample'].values\n",
    "  # To eliminate the ID_sample from the spectrum\n",
    "  spectrum_test_forcsv = spectrum_test_forcsv.drop(columns=['ID_sample'])\n",
    "  for c, cat in enumerate(categories): \n",
    "      # clean NaN values\n",
    "      X_train, Y_train = clean_nan_samples(spectrum_full_train,targets_full_train, c, cat)\n",
    "\n",
    "      # fit the classifier\n",
    "      clf_base = clf_list[c].fit(X_train,Y_train)\n",
    "      # Compute its test prestiction and save this output\n",
    "      o_test = clf_base.predict_proba(spectrum_test_forcsv)[:,1]\n",
    "      df_submission[cat] = o_test\n",
    "\n",
    "  # Save the dataframe with the predicted outputs\n",
    "  df_submission = df_submission.set_index('ID')\n",
    "  df_submission.to_csv(path_results + file_name + '.csv')\n",
    "  print('DONE!')\n",
    "  return df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create manually a list of good classifiers\n",
    "clf_oxalacina       = #\n",
    "clf_amikacina       = #\n",
    "clf_amoxiclav       = #\n",
    "clf_ciprofloxacino  = #\n",
    "clf_clindamicina    = #\n",
    "clf_eritromicina    = #\n",
    "clf_levofloxacino   = #\n",
    "clf_penicilina      = #\n",
    "clf_tobramicina     = #\n",
    "\n",
    "clf_list = [clf_oxalacina, clf_amikacina, clf_amoxiclav, clf_ciprofloxacino, clf_clindamicina,\n",
    "            clf_eritromicina, clf_levofloxacino, clf_penicilina, clf_tobramicina]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'filename'\n",
    "df_submission = generate_csv_from_clf(clf_list,path,path_results, name)\n",
    "print('File: '+name+' has been successfully generated')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
