{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import _pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "Remember to change path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path = \"D:/GitHub/Machine-Learning/Kaggle/\"\n",
    "path = \"C:/Users/Javi/Documents/GitHub/Machine-Learning/Kaggle/\"\n",
    "\n",
    "zf = zipfile.ZipFile(path+'zipped_TrainData.zip', 'r')\n",
    "df_train = _pickle.loads(zf.open('TrainData.pkl').read())\n",
    "zf.close()\n",
    "\n",
    "zf = zipfile.ZipFile(path+'zipped_TestDataUnlabeled.zip', 'r')\n",
    "df_test = _pickle.loads(zf.open('TestDataUnlabeled.pkl').read())\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split & Spectrum in regular bins\n",
    "Data is split between a proper training set (later used in cross-validation), and a test_train set, which will help us in determining under/overfitting as we do have labels for them.\n",
    "\n",
    "Spectrums are divided in regular size bins, always the same, so that we can treat them as features, not worrying about different mz scales. According to the literature the peaks contain the relevant information, then we only save the maximum value in the bin (range of mz coordinates) so that peak information is never lost. Moreover, by performing this regularization in bins, peaks at very close mz values (same compound, small mz differences due to experimental uncertainty) are seen by the machine as belonging to the same bin and therefore the same feature. Therefore, it facilitates to use peaks as values.\n",
    "\n",
    "Also, peak values are normalized by the maximum peak value of the spectrum, as specific values are experiment-dependent and do not carry information, only the relation between peak sizes does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spectrum_in_bins(df,m,M,bin_size):\n",
    "    # Now, let's define the mz ranges, and the label associated to each of them (the mean of the limiting values of each bin)\n",
    "    range_min = []; range_max = []; range_label = [];\n",
    "    for mz in range(m,M,bin_size):\n",
    "        range_min.append(mz)\n",
    "        range_max.append(mz+bin_size)\n",
    "        range_label.append(np.mean([range_min[-1],range_max[-1]]).astype(int))\n",
    "\n",
    "\n",
    "    N = len(df)  # number of samples\n",
    "    L = len(range_min)  # length of new spectrum (number of bins)\n",
    "    all_data = np.zeros((N,L))\n",
    "    for idx in range(N): \n",
    "        intensity = df[['intensity']].iloc[idx].values[0]\n",
    "        mzcoord   = df[['coord_mz']].iloc[idx].values[0]\n",
    "        idx_data_in_bins = np.zeros((1,L))\n",
    "        for i,mz in enumerate(range_min):\n",
    "            intensity_range = intensity[(mzcoord > mz) & (mzcoord < (mz+bin_size))]\n",
    "            if len(intensity_range) > 0 :\n",
    "                idx_data_in_bins[0,i] = np.max(intensity_range)\n",
    "            else: # if those mz coordinates are not in that spectrum\n",
    "                idx_data_in_bins[0,i] = 0   \n",
    "\n",
    "        # Normalize the amplitude of the spectrum\n",
    "        idx_data_in_bins[0,:] = idx_data_in_bins[0,:] / np.max(idx_data_in_bins[0,:])\n",
    "        \n",
    "        \n",
    "        all_data[idx,:] = idx_data_in_bins\n",
    "    new_df = pd.DataFrame(data=all_data, columns = range_label, index = df.index)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 412 and test_train samples: 104\n",
      "Spectrum regularized!\n"
     ]
    }
   ],
   "source": [
    "# Extract data (spectra) and targets of the df_train set\n",
    "data = df_train.iloc[:,-2:]\n",
    "targets = df_train.iloc[:,1:-2]\n",
    "\n",
    "m = 2000; M = 20000; \n",
    "bin_size = 50;\n",
    "\n",
    "# Then, split into a train and test_train set\n",
    "data_train, data_test_train, targets_train, targets_test_train = train_test_split(data, targets, test_size=0.2, random_state=42) # split the data\n",
    "print('Training samples: '+str(len(data_train))+' and test_train samples: ' + str(len(data_test_train)) )\n",
    "\n",
    "# apply the bins to all spectra, so that our feature space becomes the same for all samples (make them regular, all the same)\n",
    "spectrum_train = spectrum_in_bins(data_train,m,M,bin_size)\n",
    "spectrum_test_train = spectrum_in_bins(data_test_train,m,M,bin_size)\n",
    "print('Spectrum regularized!')\n",
    "# these spectrum_... are our X for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different classifiers\n",
    "The try_clf function has been built for, given a classifier and a parameter dictionary (for hyperparameter cross-validation), create a classifier for each antibiotic, and return the results. This enables for fast testing of different classifiers. Moreover, the function also takes charge of suppressing NaN values in the targets ocurring for amikacina, levofloxacino and tobramicina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def try_clf(clf,params,n_cv=5):\n",
    "    t1 = time.time()\n",
    "    \n",
    "    best_classifiers = [];\n",
    "    accuracies_train = []; accuracies_test_train = [];\n",
    "    AUC_train = []; AUC_test_train = [];\n",
    "    \n",
    "    # Selection of train and test data (depending on whether there are NaN target values)\n",
    "    for antibiotic_idx in range(len(targets_train.columns.values)):\n",
    "        column_name = [df_train.columns.values[1+antibiotic_idx]]\n",
    "        \n",
    "        print()\n",
    "        print(column_name) # indicate in which antibiotic we are\n",
    "        \n",
    "        if (df_train[column_name].isnull().sum() > 0).all(): # if there are NaN values, we should remove those samples\n",
    "#             print('There are NaN values in',column_name)\n",
    "            merged_train = pd.concat([spectrum_train , targets_train],axis=1,copy=True)\n",
    "            merged_test_train = pd.concat([spectrum_test_train, targets_test_train],axis=1)\n",
    "            clean_train = merged_train.dropna(subset=column_name)\n",
    "#             print('Dropped ',len(merged_train)-len(clean_train),' from training set')\n",
    "            clean_test_train = merged_test_train.dropna(subset=column_name)\n",
    "#             print('Dropped ',len(merged_test_train)-len(clean_test_train),' from test_training set')\n",
    "\n",
    "            Y_train = clean_train.iloc[:,-9+antibiotic_idx].to_numpy().reshape(-1,)\n",
    "            Y_test_train = clean_test_train.iloc[:,-9+antibiotic_idx].to_numpy().reshape(-1,)\n",
    "            X_train = clean_train.iloc[:,:-9]\n",
    "            X_test_train = clean_test_train.iloc[:,:-9]\n",
    "        else:\n",
    "            Y_train = targets_train.iloc[:,antibiotic_idx].to_numpy().reshape(-1,)\n",
    "            Y_test_train = targets_test_train.iloc[:,antibiotic_idx].to_numpy().reshape(-1,)\n",
    "            X_train = spectrum_train.copy(deep=True)\n",
    "            X_test_train = spectrum_test_train.copy(deep=True)\n",
    "            \n",
    "        # perform a GridSearchCV in order to train a classifier for this antibiotic\n",
    "        grid = GridSearchCV(clf,param_grid=params, cv=n_cv, iid=False)\n",
    "        grid.fit(X_train, Y_train)\n",
    "\n",
    "        # print the best parameters (to detect edge values), and save that classifier\n",
    "        print('The best parameters are: ',grid.best_params_)\n",
    "        best_clf = grid.best_estimator_\n",
    "        best_classifiers.append(best_clf)\n",
    "        \n",
    "        # compute the accuracy of the classifier\n",
    "        acc_train = best_clf.score(X_train, Y_train)\n",
    "        acc_test = best_clf.score(X_test_train, Y_test_train)\n",
    "        print('Train accuracy: ',np.round(acc_train,4),' and test_train accuracy: ',np.round(acc_test,4))\n",
    "        accuracies_train.append(acc_train)\n",
    "        accuracies_test_train.append(acc_test)\n",
    "        \n",
    "        # compute the AUC of the classifier\n",
    "        if callable(getattr(best_clf,\"predict_proba\",None)):\n",
    "            pred_train = best_clf.predict_proba(X_train)[:,-1] # only take last column, the prob of Y = +1\n",
    "            pred_test = best_clf.predict_proba(X_test_train)[:,-1]\n",
    "        else:\n",
    "            print('Using decision_function instead of predict_proba')\n",
    "            pred_train = best_clf.decision_function(X_train)\n",
    "            pred_test = best_clf.decision_function(X_test_train)            \n",
    "        auc_score_train = roc_auc_score(Y_train, pred_train)\n",
    "        auc_score_test = roc_auc_score(Y_test_train, pred_test)\n",
    "        print('Train AUC: ',np.round(auc_score_train,4),' and test_train AUC: ',np.round(auc_score_test,4))\n",
    "        AUC_train.append(auc_score_train)\n",
    "        AUC_test_train.append(auc_score_test)\n",
    "        \n",
    "    avg_AUC_train = np.mean(AUC_train)\n",
    "    avg_AUC_test_train = np.mean(AUC_test_train)\n",
    "    print('\\n\\nThe average train AUC is',np.round(avg_AUC_train,4),'and the avg test_train AUC is',np.round(avg_AUC_test_train,4))\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print('\\nFull execution took ',np.round(t2-t1,1),'seconds')\n",
    "    print('\\nDONE!')\n",
    "    return best_classifiers, accuracies_train, accuracies_test_train, AUC_train, AUC_test_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regressor classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['OXACILINA']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.8228  and test_train accuracy:  0.75\n",
      "Train AUC:  0.9086  and test_train AUC:  0.7906\n",
      "\n",
      "['AMIKACINA']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.8468  and test_train accuracy:  0.7683\n",
      "Train AUC:  0.8958  and test_train AUC:  0.706\n",
      "\n",
      "['AMOXI/CLAV']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.8301  and test_train accuracy:  0.75\n",
      "Train AUC:  0.9063  and test_train AUC:  0.7771\n",
      "\n",
      "['CIPROFLOXACINO']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.835  and test_train accuracy:  0.7692\n",
      "Train AUC:  0.8871  and test_train AUC:  0.7741\n",
      "\n",
      "['CLINDAMICINA']\n",
      "The best parameters are:  {'C': 1e-05}\n",
      "Train accuracy:  0.7961  and test_train accuracy:  0.7788\n",
      "Train AUC:  0.6269  and test_train AUC:  0.6991\n",
      "\n",
      "['ERITROMICINA']\n",
      "The best parameters are:  {'C': 1000.0}\n",
      "Train accuracy:  0.8544  and test_train accuracy:  0.6827\n",
      "Train AUC:  0.9016  and test_train AUC:  0.7049\n",
      "\n",
      "['LEVOFLOXACINO']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.8195  and test_train accuracy:  0.7647\n",
      "Train AUC:  0.8989  and test_train AUC:  0.7667\n",
      "\n",
      "['PENICILINA']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.915  and test_train accuracy:  0.9135\n",
      "Train AUC:  0.8947  and test_train AUC:  0.816\n",
      "\n",
      "['TOBRAMICINA']\n",
      "The best parameters are:  {'C': 100.0}\n",
      "Train accuracy:  0.8462  and test_train accuracy:  0.747\n",
      "Train AUC:  0.8977  and test_train AUC:  0.7547\n",
      "\n",
      "\n",
      "The average train AUC is 0.8686 and the avg test_train AUC is 0.7544\n",
      "\n",
      "Full execution took  20.0 seconds\n",
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter=1e6, solver='lbfgs')\n",
    "params = {'C':10.**np.arange(-5,5)}\n",
    "lr_best_clfs, _, _, lr_AUC_train, lr_AUC_test_train = try_clf(clf,params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
