{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Regression with Gaussian Processes\n",
    "\n",
    "------------------------------------------------------\n",
    "*Machine Learning, Master in Information Health Engineering, 2019-2020*\n",
    "\n",
    "*Pablo M. Olmos olmos@tsc.uc3m.es*\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "The aim of this homework is to solve a real data problem using the Gaussian Process implementation of GPy. The documentation of GPy is avaialable from the [SheffieldML github page](https://github.com/SheffieldML/GPy) or from [this page](http://gpy.readthedocs.org/en/latest/). \n",
    "\n",
    "The problem is the prediction of both the heating load (HL) and cooling load (CL) of residential buildings. We consider eight input variables for each building: relative compactness, surface area, wall area, roof area, overall height, orientation, glazing area, glazing area distribution.\n",
    "\n",
    "In this [paper](https://www.sciencedirect.com/science/article/pii/S037877881200151X) you can find a detailed description of the problem and a solution based on linear regression [(iteratively reweighted least squares (IRLS) algorithm)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=10&ved=2ahUKEwjZuoLY2OjgAhUs3uAKHUZ7BVcQFjAJegQIAhAC&url=https%3A%2F%2Fpdfs.semanticscholar.org%2F9b92%2F18e7233f4d0b491e1582c893c9a099470a73.pdf&usg=AOvVaw3YDwqZh1xyF626VqfnCM2k) and random forests. Using GPs, our goal is not only estimate accurately both HL and CL, but also get a measure of uncertainty in our predictions.\n",
    "\n",
    "The data set can be downloaded from the [UCI repository](https://archive.ics.uci.edu/ml/datasets/Energy+efficiency#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and preparing the data\n",
    "\n",
    "* Download the dataset\n",
    "* Divide at random the dataset into train (80%) and test (20%) datasets \n",
    "* Normalize data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import GPy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data from the website\n",
    "data = pd.read_excel('https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data looks like:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
       "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
       "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
       "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
       "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
       "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The data looks like:')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1    0\n",
      "X2    0\n",
      "X3    0\n",
      "X4    0\n",
      "X5    0\n",
      "X6    0\n",
      "X7    0\n",
      "X8    0\n",
      "Y1    0\n",
      "Y2    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "empty = data.apply(lambda col: pd.isnull(col)).sum()\n",
    "print(empty) # check whether there are missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 768 data samples\n"
     ]
    }
   ],
   "source": [
    "# Check how many data points we have\n",
    "print('There are %d data samples' %data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,0:8]\n",
    "\n",
    "Y = data.iloc[:,8:10]\n",
    "\n",
    "feature_names = {\n",
    "    'X1': 'Relative Compactness',\n",
    "    'X2': 'Surface Area',\n",
    "    'X3': 'Wall Area',\n",
    "    'X4': 'Roof Area',\n",
    "    'X5': 'Overall Height',\n",
    "    'X6': 'Orientation',\n",
    "    'X7': 'Glazing Area',\n",
    "    'X8': 'Glazing Area Distribution' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and normalize the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # split the data\n",
    "\n",
    "# split target between HL and CL\n",
    "HL_train = Y_train[['Y1']].to_numpy().reshape(-1,1)\n",
    "CL_train = Y_train[['Y2']].to_numpy().reshape(-1,1)\n",
    "HL_test = Y_test[['Y1']].to_numpy().reshape(-1,1)\n",
    "CL_test = Y_test[['Y2']].to_numpy().reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "transformer = StandardScaler() \n",
    "transformer.fit(X_train) # fit the transformer\n",
    "X_train_norm = transformer.transform(X_train) # transform\n",
    "X_test_norm =  transformer.transform(X_test) \n",
    "X_train_norm = pd.DataFrame(X_train_norm,index=X_train.index, columns=X_train.columns) # reconvert to DataFrames\n",
    "X_test_norm = pd.DataFrame(X_test_norm,index=X_test.index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting and optimizing the model\n",
    "\n",
    "You will train two independent GPs, one to estimate HL and one to estimate CL. For each of the two GPs ...\n",
    "\n",
    "**On the training data set:**\n",
    "\n",
    "a) Build a GP regression model based on a RBF kernel with ARD, in which each input dimension is weighted with a different lengthscale, plus some constant noise term. \n",
    "\n",
    "b) Fit the covariance function parameters and noise variance. \n",
    "\n",
    "c) According to the ARD parameters found, what variables are more important for the regression? Compare it to Table 8 in the [paper](https://www.sciencedirect.com/science/article/pii/S037877881200151X) \n",
    "\n",
    "**On the test data set:**\n",
    "\n",
    "d) Compute the test mean absolute error error and the test mean square error (MSE)  using the GP posterior mean and the optimized hyperparameters. Compare your results with Tables 6 and 7 in the [paper](https://www.sciencedirect.com/science/article/pii/S037877881200151X).\n",
    "\n",
    "\n",
    "2) Try to improve your results by using a more complicated kernel, in which you combine various covariance functions. In this [link](http://nbviewer.jupyter.org/github/SheffieldML/notebook/blob/master/GPy/basic_kernels.ipynb) you can see how to define different kernels and combine  them. Comments the results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heating Load GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " C:\\Users\\Javi\\Anaconda3\\lib\\site-packages\\GPy\\kern\\src\\stationary.py:166: RuntimeWarning:overflow encountered in true_divide\n",
      " C:\\Users\\Javi\\Anaconda3\\lib\\site-packages\\GPy\\kern\\src\\stationary.py:137: RuntimeWarning:overflow encountered in square\n",
      " C:\\Users\\Javi\\Anaconda3\\lib\\site-packages\\GPy\\kern\\src\\stationary.py:138: RuntimeWarning:invalid value encountered in add\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/5, f = 542.7391213722025\n",
      "Optimization restart 2/5, f = 550.8583756888372\n",
      "Optimization restart 3/5, f = 535.0416417842602\n",
      "Optimization restart 4/5, f = 540.1077741831721\n"
     ]
    }
   ],
   "source": [
    "n_features = 8\n",
    "kernelHL = GPy.kern.RBF(n_features,variance=1.0, lengthscale=1.0, ARD=True) + GPy.kern.White(n_features)\n",
    "mHL = GPy.models.GPRegression(X_train_norm, HL_train, kernelHL)\n",
    "mHL.optimize_restarts(num_restarts = 5)\n",
    "\n",
    "\n",
    "# trains faster, but yields incorrect results (much higher MAE and MSE)\n",
    "# mHL2 = GPy.models.GPRegression(X_train_norm, Y_train[['Y1']], kernelHL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kernelHL.parameter_names())\n",
    "print(kernelHL.rbf.variance.values)\n",
    "print(kernelHL.rbf.lengthscale.values)\n",
    "print(kernelHL.white.variance.values)\n",
    "\n",
    "idx = np.argsort(kernelHL.rbf.lengthscale.values) + 1 # add 1 so that indices correspond to X1...X8\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanHLtest,_ = mHL.predict(X_test_norm.to_numpy().reshape(-1,n_features))\n",
    "from sklearn.metrics import mean_absolute_error , mean_squared_error #as MAE , MSE\n",
    "MAE1 = mean_absolute_error(HL_test,meanHLtest)\n",
    "MSE1 = mean_squared_error(HL_test,meanHLtest)\n",
    "print('MAE is %.2f and MSE is %.2f' %(MAE1,MSE1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(mHL)\n",
    "# fig = mHL.plot(plot_density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse GP implementation \n",
    "\n",
    "Try to implement an sparse version of the GP regressor, optimized to find a set of **inducing points** that the GP relies on to do the prediction. Measure the test error prediction for 20, 40, and 100 inducing points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
