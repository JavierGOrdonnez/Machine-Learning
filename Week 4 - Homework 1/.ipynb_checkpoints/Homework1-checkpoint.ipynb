{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-jkWR1LhNMh"
   },
   "source": [
    "# Week 4: Homework 1 \n",
    "\n",
    "----------------------------------------------------\n",
    "Machine Learning                      \n",
    "\n",
    "Year 2019/2020\n",
    "\n",
    "*Vanessa Gómez Verdejo vanessa@tsc.uc3m.es* \n",
    "\n",
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfXCi3qshchx"
   },
   "source": [
    "The goal of this practice is to analyze the performance of different estimators on the Diabetes problem and we will analyze, by means of different approximations, which input features are more relevant to solve this problem. \n",
    "\n",
    "Note that previous week we already worked on Diabetes database, but we only used one of the input variable (BMI) to construct the regression model; in this practice we will use all the input features jointly.\n",
    "\n",
    "To solve these notebook, complete the following sections implementing the solution that you consider most appropriate and showing the results that you find most interesting. For the evaluation of this notebook,  we will take into account the methodology used, the solution adopted, the presentation of the results and the conclusions obtained at the light of the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCXU4m2YmB2y"
   },
   "source": [
    "## 1. Data loading and preprocessing\n",
    "\n",
    "Following the ML pipeline, start loading the data, creating the partitions that you consider necessary and carrying out the preprocessing that yu need.\n",
    "\n",
    "Keep in mind that there is no single valid solution, and different reasons can lead you to make different data partitions or apply different normalizations. So **please justify the steps you are taking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7fqumVhhJAl"
   },
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "\n",
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zemNGztJfH-Z"
   },
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "Y = diabetes.target\n",
    "feature_names = diabetes.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZT8rXwbXfiBz"
   },
   "outputs": [],
   "source": [
    "# Generate train and test partitions\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Data normalization\n",
    "transformer = StandardScaler().fit(X_train)  # fit() learns mean and std parameters\n",
    "X_train = transformer.transform(X_train) # transform() normalizes\n",
    "X_test =  transformer.transform(X_test)\n",
    "\n",
    "# Generate validation set\n",
    "####\n",
    "#### Let's better use GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fe7_QAXDmatm"
   },
   "source": [
    "## 2. Performance evaluation \n",
    "\n",
    "Now, analyze the performance of different estimators to predict the diabetes progression from all the available features. \n",
    "\n",
    "As possible estimators to be included in this study, we will consider those studied so far: K-NN, linear regressor, polynomial regressor and their regularized versions. Please, in case these methods have any free hiperparameter, **clearly justify** the selection of their optimal values.\n",
    "\n",
    "As you know, for the performance evaluation, we have seen several metrics. So, here, you can use one or several of them. But, regardless of the used metric, don't forget to include a final analysis comparing the performance of different methods and trying to justify the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EnOY0K4Fo62S"
   },
   "outputs": [],
   "source": [
    "## 2. Performance evaluation \n",
    "\n",
    "# Include your code here (create as many cells as you need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsPte6LjhO00"
   },
   "source": [
    "### 2.1 K-NN\n",
    "First of all, let's use the K-NN estimator in order to try to predict disease progression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "id": "2Z9S0szahV3M",
    "outputId": "76bcbafd-a4c1-4763-ac4d-53d5015fa40f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal k value is 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=14, p=2,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a KNN object from th SKlearn module\n",
    "neigh = KNeighborsRegressor()  \n",
    "\n",
    "# Perform a Cross-Validation in order to choose the hiperparameter K\n",
    "rang_k = np.arange(1,50)\n",
    "parameters = {'n_neighbors' : rang_k}\n",
    "clf = GridSearchCV(neigh,parameters,iid=False,cv=10, scoring = 'neg_mean_squared_error')\n",
    "clf.fit(X_train,Y_train) # this executes the cross-validation\n",
    "\n",
    "# Visualize the results of CrossValidation\n",
    "sqerrors_k = clf.cv_results_['mean_test_score']*-1\n",
    "optimk = clf.best_params_['n_neighbors']\n",
    "plt.figure()\n",
    "plt.plot(rang_k, sqerrors_k)\n",
    "plt.title('Mean square error for different K values')\n",
    "plt.show()\n",
    "print('The optimal k value is %d' %optimk)\n",
    "\n",
    "# The KNN estimator, already trained with the optimal K\n",
    "neigh = clf.best_estimator_\n",
    "neigh.fit(X_train,Y_train) # and provide our data to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzoX4HcsnuZu"
   },
   "source": [
    "For evaluation of the method, R2 score is going to be used. It is the most used metric, as it is easy to interpret: 1.00 would mean no error; 0.00 would mean predicting always the mean, and negative would be performing even worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lxESgYieloZx",
    "outputId": "458ad67d-42d2-439f-9606-dc6bdc858578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score of the KNN method is 0.329\n"
     ]
    }
   ],
   "source": [
    "# Let's now evaluate the performance of the KNN regressor over the test data\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "knn_prediction = neigh.predict(X_test)\n",
    "knn_score = r2_score(Y_test,knn_prediction)\n",
    "print('The R2 score of the KNN method is %.3f' %knn_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZjAMKXTQhWcN"
   },
   "source": [
    "### 2.2. Linear regressor (and regularized versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "bYh5FKUchbTE",
    "outputId": "4381fcfb-5548-46fd-e073-bb9787b2b9e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R2 score of the linear regressor is 0.332\n",
      "The R2 score of the Lasso regressor is 0.334. Optimal α = 1.000000.\n",
      "The optimal alpha for the Lasso regressor is 1.000000\n",
      "The R2 score of the Ridge regressor is 0.331. Optimal α = 10.000000.\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model as ln\n",
    "\n",
    "# Let's train several different linear regressors, and find their score\n",
    "\n",
    "# Ordinary least squares linear regression\n",
    "linear = ln.LinearRegression(fit_intercept=True).fit(X_train,Y_train)\n",
    "linear_prediction = linear.predict(X_test)\n",
    "linear_score = r2_score(Y_test,linear_prediction)\n",
    "print('The R2 score of the linear regressor is %.3f' %linear_score)\n",
    "\n",
    "# Lasso. Needed to cross-validate alpha\n",
    "lasso = ln.Lasso(fit_intercept=True)\n",
    "rang_alpha = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "parameters = {'alpha' : rang_alpha}\n",
    "clf = GridSearchCV(lasso,parameters,iid=False,cv=10, scoring= 'neg_mean_squared_error')\n",
    "clf.fit(X_train,Y_train)\n",
    "lasso = clf.best_estimator_\n",
    "lasso_prediction = lasso.predict(X_test)\n",
    "lasso_score = r2_score(Y_test,lasso_prediction)\n",
    "print('The R2 score of the Lasso regressor is %.3f. Optimal \\u03B1 = %f.' %(lasso_score,clf.best_params_['alpha']))\n",
    "print('The optimal alpha for the Lasso regressor is %f' %clf.best_params_['alpha'])\n",
    "\n",
    "# Ridge. Needed to cross-validate alpha\n",
    "rang_alpha = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "parameters = {'alpha' : rang_alpha}\n",
    "clf = GridSearchCV(ln.Ridge(fit_intercept=True),parameters,iid=False,cv=10, scoring= 'neg_mean_squared_error')\n",
    "clf.fit(X_train,Y_train)\n",
    "ridge = clf.best_estimator_\n",
    "ridge_prediction = ridge.predict(X_test)\n",
    "ridge_score = r2_score(Y_test,ridge_prediction)\n",
    "print('The R2 score of the Ridge regressor is %.3f. Optimal \\u03B1 = %f.' %(ridge_score,clf.best_params_['alpha']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1RgTm6RhbjE"
   },
   "source": [
    "### 2.3. Polynomial regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HePjfzL4g2uB"
   },
   "outputs": [],
   "source": [
    "# Let's define a class which contains the polynomial regressor\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "class PolyReg:\n",
    "    def __init__(self,n_degree,regressor='linear'):\n",
    "    \n",
    "        self.n_degree = n_degree\n",
    "\n",
    "        if   regressor == 'linear':\n",
    "            self.regressor = ln.LinearRegression(fit_intercept=False)\n",
    "        elif regressor == 'Lasso':\n",
    "            self.regressor = ln.Lasso(fit_intercept=False)\n",
    "        elif regressor == 'Ridge':\n",
    "            self.regressor = ln.Ridge(fit_intercept=False)  \n",
    "  \n",
    "  \n",
    "    def score(self,X,Y): # implement neg MSE\n",
    "        score = self.regressor.score(X,Y)\n",
    "        return score\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "    \n",
    "        # Define the polynomial features\n",
    "        self.polynomial_features = PolynomialFeatures(degree=self.n_degree,include_bias=True)\n",
    "        X_poly = self.polynomial_features.fit_transform(X)  # We will find the polynomial degree that fits best for each regularization, and then we will evaluate them\n",
    "        self.transformer = StandardScaler().fit(X_poly[:,1:])\n",
    "        X_poly[:,1:] = self.transformer.transform(X_poly[:,1:])\n",
    "\n",
    "        # Fit the regressor with the polynomial features\n",
    "        self.regressor.fit(X_poly,Y)  \n",
    "\n",
    "        return self.regressor\n",
    "    \n",
    "    def predict(self,X): \n",
    "        X_poly = self.polynomial_features.fit_transform(X)\n",
    "        X_poly[:,1:] = self.transformer.transform(X_poly[:,1:])\n",
    "        prediction = self.regressor.predict(X_poly)\n",
    "        return prediction\n",
    "\n",
    "    def eval(self,X,Y): # evaluate with r2 score\n",
    "        # Transform the evaluation input in a polynomial set of features, and normalize them\n",
    "        # according to the transformations set during training\n",
    "        X_poly = self.polynomial_features.fit_transform(X)\n",
    "        X_poly[:,1:] = self.transformer.transform(X_poly[:,1:])\n",
    "        \n",
    "        # Obtain predictions with the trained regressor\n",
    "        predictions = self.regressor.predict(X_poly)\n",
    "        \n",
    "        # Compute test and train R2 scores\n",
    "        eval_score  = metrics.r2_score(Y,predictions)\n",
    "        return eval_score\n",
    "  \n",
    "    def return_regressor(self):\n",
    "        return self.regressor\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3322220326906514"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test that it yields the same results than the previous section (with n_degree = 1)\n",
    "myLinear = PolyReg(n_degree=1,regressor = 'linear')\n",
    "myLasso  = PolyReg(n_degree=1,regressor = 'Lasso')\n",
    "myRidge  = PolyReg(n_degree=1,regressor = 'Ridge')\n",
    "\n",
    "\n",
    "myPoly.fit(X_train,Y_train)\n",
    "myPoly.eval(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKvDZ_FahsDl"
   },
   "source": [
    "### Regularized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxCrlnE0huMt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VHWjw1eAo_9V"
   },
   "source": [
    "## 3. Study of feature relevance and feature selection\n",
    "\n",
    "In this last section, using different criteria, you have to analyze the relevance of the input features. Thus, you will have to find a subset with the $D'$ most relevant features and, using this subset of features, analyze the final performance of a regressor (for the sake of simplicity, consider a linear ridge regressor as final regressor).\n",
    "\n",
    "To analyze the feature importances or relevances, you can use the following criteria:\n",
    "\n",
    "1. **Relevance ranking based on the validation error**: if there were $D$ input features, we could try to train $D$ regressors where each regressor uses one (and only one) different input feature. According to the final perfomance of each regressor (evaluated on a validation set or with a CV proccess), we could rank the features (the most relevant feature is the one providing the lowest error). Using this ranking, we can select the $D'$ most relevant features as the $D'$ top-ranked ones. Note that this scheme only analyzes the isolated relevance of each feature to predict the output; so, it is said that this approach is *univariate*.\n",
    "\n",
    " <img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Regression/FeatureRanking.png\" width=\"90%\" > \n",
    "\n",
    "\n",
    "2. **Greedy search based on the validation error**: approach (1) has the disadvantage of not taking into account relationships between features. For instance, method (1) would not realize that two features can be rendundant or that a feature, that is useless by itself, can be very useful combined with another feature. To overcome this drawback, we should have to analyze subsets of features; however, exploring all possible subsets  is usually computationally unflexible (there are $2^D$ combinations!!!!); so a greedy search (fordward or backward) is usually prefered:\n",
    "\n",
    "  2.1 *Fordward search*: It starts with an empty set and, iteratively, adds new features according to a relevance criterion (in this case, minimum validation MSE).\n",
    "\n",
    "  <img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Regression/Fig_Forward_search.png\" width=\"48%\" > \n",
    "\n",
    "  2.2 *Backward search*: It starts considering all the features and, iteratively, removes features according to a relevance criterion (in this case, minimum validation MSE).\n",
    "\n",
    "  <img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Regression/Fig_Backward_search.png\" width=\"48%\" > \n",
    "\n",
    "3. **Ridge linear regression with a prunning**: We know that the L2 regularization limits the magnitude of the weight vector to avoid overfitting problems, but these weigths do not become to null. However, in a linear model, *the weight magnitude can be an indicative of the feature relevance* and, unlike approach (1), all features are analyzed at the same time (*multivariate approach*). Use the weigth magnitude to generate a ranking of features and, later, use this ranking to select the $D'$ most relevant features.\n",
    "\n",
    "4. **Lasso linear regression**: In this case, the L1 penalty allows us to directly eliminate some of the input features. Explore different values of the regularization parameter $\\lambda$ to get a sequence of selected feature sets (from a single feature to all features).\n",
    "\n",
    "\n",
    "5. **Elastic-net linear regression**: This last approach combines L1 and L2 regularizations, thus including the advantages of both methods. Varying adequately their regularization parameters, create a sequence of feature selection subsets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fe2uvnVP0-AZ"
   },
   "source": [
    "Final comments:\n",
    "\n",
    "* Due to part of the feature selection process involves selecting the optimum number of features, to avoid additional complexity (having to validate this number), you can analyze the different methods by exploring the curves of MSE vs. number of selected features ($D'$).\n",
    "\n",
    "* It is not necessary to apply all these methods to complete this notebook (you can choose, at least, three of them). In fact, the implementation of greedy search approaches require an advanced knowledge of Python; so take this into account when you design your notebook solution.\n",
    "\n",
    "* **Please, analyze in detail the different results, pointing out the advantages/disadvantages of each feature selection scheme**. Think about the behaviour of the different criteria in cases where a feature is irrelevant or there are redundant features. Additional experiments helping you to support any of your conclusions will be welcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtDSa1iq-G9V"
   },
   "outputs": [],
   "source": [
    "## 3. Feature selection \n",
    "\n",
    "# Include your code here (create as many cells as you need)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
